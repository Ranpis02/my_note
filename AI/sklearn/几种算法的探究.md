# 1 决策树

## 1.1 基本概念

**信息量、信息熵和信息增益**

| 概念                   | 定义                                                         | 意义                                                         |
| ---------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 信息量(info)           | $inf\ o=-plog_2p$，某一特征下，特征为指定值时的信息量        |                                                              |
| 信息熵（Entropy）      | $\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y|}} p_k \log _2 p_k$，某一特征下，特征取遍所有值时的总信息量 | 信息熵用来衡量数据集的不确定性，信息熵越大，数据的不确定性越大，所包含的信息越多 |
| 信息增益（Gain）       | $$\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^V \frac{\left|D^v\right|}{|D|} \operatorname{Ent}\left(D^v\right)$$，信息熵的有效减少量 | 信息增益越大，说明信息熵的减少越多，不确定减少越多           |
| 信息增益率(Gain_Ratio) | $Gain\_Ratio(D, a)=\frac{Gain(D, a)}{IV(a)}$，$IV(a)$ 代表分裂信息 |                                                              |



**距离指标**

<table>
<tr>
    <th>欧氏距离(Euclidean distance)</th>
    <th>曼哈顿距离(Manhattan distance)</th>
    <th>余弦距离(Cosine distance)</th>
</tr>
<tr>
<td><img src='https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240830084856821.png' border=0></td>
<td><img src='https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240830085345912.png' border=0></td>
<td><img src='https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240830085706804.png' border=0></td>
</tr>
</table>

其中余弦距离（也称为余弦相似度）大小为：
$$
s(p,q)=\cos(p,q)=\frac{p\cdot q}{\parallel p\parallel_{2}\cdot\parallel q\parallel_{2}}=\frac{\sum_{i=1}^{m}p_{i}\cdot q_{i}}{\sqrt{\sum_{i=1}^{m}p_{i}^{2}\cdot\sum_{i=1}^{m}q_{i}^{2}}}
$$
将余弦相似度进行标准化，其值越接近 1 或 -1 ，代表两特征相关性越强，越接近 0，相关性越弱
$$
s(p,q)=\frac{\sum_{i=1}^m(p_i-\overline{p}_i)\cdot(q_i-\overline{q}_i)}{\sqrt{\sum_{i=1}^m(p_i-\overline{p}_i)^2\cdot\sum_{i=1}^m(q_i-\overline{q}_i)^2}}
$$


## 1.2 ID3

ID3（Iterative Dichotomiser 3）是根据信息增益为准则来选择最优划分属性，由于涉及到树结构，因此我们需要通过递归完成，递归的终止条件为：

1. 当分到某个类时，目标属性全是一个值
2. 当分到某个类时，某个值的比例达到给定的阈值

> [!tip]
>
> - every element in the subset belongs to the same class; in which case the node is turned into a [leaf node](https://en.wikipedia.org/wiki/Leaf_node) and labelled with the class of the examples.
> - there are no more attributes to be selected, but the examples still do not belong to the same class. In this case, the node is made a leaf node and labelled with the [most common class](https://en.wikipedia.org/wiki/Mode_(statistics)) of the examples in the subset.
> - there are [no examples in the subset](https://en.wikipedia.org/wiki/Empty_set), which happens when no example in the parent set was found to match a specific value of the selected attribute. An example could be the absence of a person among the [population](https://en.wikipedia.org/wiki/Population_(statistics)) with age over 100 years. Then a leaf node is created and labelled with the most common class of the examples in the parent node's set.

具体实现代码

```python
import numpy as np
import pandas as pd
from math import log2
from graphviz import Digraph


# 计算熵
def entropy(d):
    labels = d.iloc[:, -1]  # 获取最后一列标签
    # value_counts 是 pandas 库中的一个方法
    label_counts = labels.value_counts()  # 统计标签的数量
    total = len(labels)  # 总样本数
    ent = -sum((count / total) * log2(count / total) for count in label_counts)
    return ent


# 计算信息增益
def information_gain(d, feature):
    # 计算总的信息熵
    total_entropy = entropy(d)
    # 计算特征信息熵
    values = d[feature].unique()
    feature_entropy = 0
    for value in values:
        # 取出 data 中 d[feature] == value 的行重新组成一个 dataframe 对象
        subset = d[d[feature] == value]
        weight = len(subset) / len(d)
        feature_entropy += weight * entropy(subset)
    gain = total_entropy - feature_entropy
    return gain


# 选择最优特征
def best_feature(d):
    features = d.columns[:-1]  # 排除最后一列（标签）
    # 生成一个字典，结构为 feature: feature_gain
    gains = {feature: information_gain(d, feature) for feature in features}
    # 找到 gains 中拥有最大值的键
    best = max(gains, key=gains.get)
    return best


# 构建决策树
def id3(d, features=None):
    # 取出最后一列数据
    labels = d.iloc[:, -1]

    # 如果所有样本属于同一类，返回该类标签
    if len(labels.unique()) == 1:
        return labels.iloc[0]

    # 如果没有特征可用，返回最多数类标签
    if features is None:
        features = d.columns[:-1]
    if len(features) == 0:
        return labels.value_counts().idxmax()

    # 选择最佳特征
    best = best_feature(d)
    tree = {best: {}}

    # 对最佳特征的每个值递归构建子树
    for value in d[best].unique():
        subset = d[d[best] == value].drop(columns=[best])
        subtree = id3(subset)
        tree[best][value] = subtree

    return tree


# 决策树可视化
def visualize_tree(tree, dot=None, parent=None, edge=None):
    if dot is None:
        dot = Digraph()
        dot.node(name='root', label=list(tree.keys())[0])
        parent = 'root'

    for edge_value, subtree in tree[list(tree.keys())[0]].items():
        node_name = str(id(subtree))
        if isinstance(subtree, dict):
            node_label = list(subtree.keys())[0]
            dot.node(name=node_name, label=node_label)
            dot.edge(parent, node_name, label=str(edge_value))
            visualize_tree(subtree, dot, node_name, edge_value)
        else:
            dot.node(name=node_name, label=str(subtree), shape='box')
            dot.edge(parent, node_name, label=str(edge_value))

    return dot


if __name__ == '__main__':
    # 示例数据集
    data = pd.DataFrame({
        'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny',
                    'Overcast', 'Overcast', 'Rain'],
        'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot',
                        'Mild'],
        'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal',
                     'High', 'Normal', 'High'],
        'Windy': [False, True, False, False, False, True, True, False, False, False, True, True, False, True],
        'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
    })

    # 构建决策树
    decision_tree = id3(data)
    print(decision_tree)

    # 可视化决策树
    dot = visualize_tree(decision_tree)
    dot.render('decision_tree', format='png', cleanup=True)  # 保存为PNG图片
    dot.view()  # 打开图片查看
```



> [!note]
>
> python 中的列表推导式、集合推导式和字典推导式
>
> - 列表推导式
>
>   ```python
>   # 基本语法
>   squares = [x**2 for x in range(10)]
>   # 等同于
>   squares = []
>   for x in range(10):
>       squares.append(x**2)
>   ```
>
> - 集合推导式
>
>   ```python
>   # 基本语法
>   unique_squares = {x**2 for x in range(10)}
>   # 等同于
>   unique_squares = set()
>   for x in range(10):
>       unique_squares.add(x**2)
>   ```
>
> - 字典推导式
>
>   ```python
>   # 基本语法
>   squares_dict = {x: x**2 for x in range(10)}
>   # 等同于
>   squares_dict = {}
>   for x in range(10):
>       squares_dict[x] = x**2	
>   ```
>
> - 带条件推导式
>
>   ```python
>   # 列表推导式，只包含偶数的平方
>   even_squares = [x**2 for x in range(10) if x % 2 == 0]
>   # 等同于
>   even_squares = []
>   for x in range(10):
>       if x % 2 == 0:
>           even_squares.append(x**2)
>   ```



## 1.3 C4.5

C4.5 基于信息增益率准则，选择最优增益率的算法。在信息增益率中，**$IV(a)$ 是由属性 a 的特征值个数决定的，个数越多，$IV(a)$ 的值越大，信息增益越小，这样就可以避免模型偏好特征值多的属性**，但是如果简单按这个规则进行划分，模型又会偏好特征值少的特征，因此 **C4.5 决策树先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的**。

同时，C4.5 算法还考虑到对连续值属性的处理，对连续型特征采用离散化技术（例如二分法），先将属性值从小到大进行排序，之后选择中间值作为分割点，数值比它小的被划分到左子树，数值比它大的被划分到右子树；也可以将考虑计算每一种分割方式的最大信息增益率，然后选择该种分割方式。

> c4.5 算法还考虑到了剪枝和缺失值处理。

|      | Outlook  | Temperature | Humidity | Windy | PlayTennis |
| :--- | :------- | :---------- | :------- | :---- | :--------- |
| 0    | Sunny    | Hot         | High     | False | No         |
| 1    | Sunny    | Hot         | High     | True  | No         |
| 2    | Overcast | Hot         | High     | False | Yes        |
| 3    | Rain     | Mild        | High     | False | Yes        |
| 4    | Rain     | Cool        | Normal   | False | Yes        |
| 5    | Rain     | Cool        | Normal   | True  | No         |
| 6    | Overcast | Cool        | Normal   | True  | Yes        |
| 7    | Sunny    | Mild        | High     | False | No         |
| 8    | Sunny    | Cool        | Normal   | False | Yes        |
| 9    | Rain     | Mild        | Normal   | False | Yes        |
| 10   | Sunny    | Mild        | Normal   | True  | Yes        |
| 11   | Overcast | Mild        | High     | True  | Yes        |
| 12   | Overcast | Hot         | Normal   | False | Yes        |
| 13   | Rain     | Mild        | High     | True  | No         |

我们以该数据集为例，通过样本标记我们可以知道这是一个二分类的数据集，标记只有 Yes 和 No

1. 计算总的信息熵
   $$
   inf\ o[9, 5] = -\frac{9}{14}log{\frac{9}{14}}-\frac{5}{14}log{\frac{5}{14}}=0.94
   $$

   > [!warning]
   >
   > 9 代表标记为"Yes"的样本个数，5 代表标记为"No"的样本个数

2. 若选择特征 Outlook 作为根节点，Outlook 的信息熵和信息增益为
   $$
   \begin{flalign}
   & E(Outlook) = inf\ o([2, 3], [4, 0], [3, 2]) = \frac{5}{14}inf\ o[2, 3] + \frac{4}{14}inf\ o[4, 0] + \frac{5}{14}inf\ o[3, 2] =0.69 \\
   & gain(Outlook) = inf\ o([9, 5]) - inf\ o([2, 3], [4, 0], [3, 2])=0.25
   \end{flalign}
   $$

3. Outlook 的信息增益率的计算方式如下，先计算分裂信息值，再与信息增益进行相除
   $$
   \begin{flalign}
   & IV(Outlook)=-\frac{5}{14}log{\frac{5}{14}}-\frac{4}{14}log{\frac{4}{14}}-\frac{5}{14}log{\frac{5}{14}}=1.577 \\
   & gain\_ratio=\frac{gain(Outlook)}{IV(Outlook)}=0.159
   \end{flalign}
   $$



代码实现

```python
import pandas as pd
import numpy as np
from math import log2
from graphviz import Digraph


# 计算熵
def entropy(d):
    labels = d.iloc[:, -1]
    label_counts = labels.value_counts()
    total = len(labels)
    ent = -sum((count / total) * log2(count / total) for count in label_counts)
    return ent


# 计算信息增益
def information_gain(d, feature):
    total_entropy = entropy(d)
    values = d[feature].unique()
    feature_entropy = 0
    for value in values:
        subset = d[d[feature] == value]
        weight = len(subset) / len(d)
        feature_entropy += weight * entropy(subset)
    gain = total_entropy - feature_entropy
    return gain


# 计算分裂信息度量
def split_info(d, feature):
    values = d[feature].unique()
    split_info = 0
    for value in values:
        subset = d[d[feature] == value]
        weight = len(subset) / len(d)
        split_info -= weight * log2(weight)
    return split_info


# 计算信息增益比
def gain_ratio(d, feature):
    gain = information_gain(d, feature)
    split_info_val = split_info(d, feature)
    if split_info_val == 0:
        return 0
    return gain / split_info_val


# 处理连续特征的最佳切分点
def best_split_point(d, feature):
    # 对特征去重后，再进行排序，去重的主要目的是为了减少信息增益率的计算
    sorted_values = sorted(d[feature].unique())
    best_gain_ratio = -1
    best_split = None
    for i in range(1, len(sorted_values)):
        split_value = (sorted_values[i - 1] + sorted_values[i]) / 2
        data_left = d[d[feature] <= split_value]
        data_right = d[d[feature] > split_value]
        gain_ratio_val = (information_gain(data_left, feature) + information_gain(data_right, feature)) / split_info(
            d, feature)
        if gain_ratio_val > best_gain_ratio:
            best_gain_ratio = gain_ratio_val
            best_split = split_value
    return best_split


# 选择最优特征
def best_feature(d):
    features = d.columns[:-1]
    best_feature = None
    best_gain_ratio = -1
    best_split_value = None
    for feature in features:
        # 以下操作进行连续特征处理
        # dtype 取出numpy对象中数据的数据类型，kind 则是将其简化为一个大的分类，例如 int64 和 int32 简化为 i, float64 和 float32 简化为 f
        if d[feature].dtype.kind in 'if':
            split_value = best_split_point(d, feature)
            if split_value is not None:
                # data[feature] <= split_value 返回一组由 True 和 False 的列表
                d['{}_leq_split'.format(feature)] = d[feature] <= split_value
                # 得到连续型变量的信息增长率
                gain_ratio_val = gain_ratio(d, '{}_leq_split'.format(feature))
                # 得到之后将其从数据集中删除
                d.drop(columns=['{}_leq_split'.format(feature)], inplace=True)

                if gain_ratio_val > best_gain_ratio:
                    best_gain_ratio = gain_ratio_val
                    best_feature = feature
                    best_split_value = split_value
        else:
            gain_ratio_val = gain_ratio(d, feature)
            if gain_ratio_val > best_gain_ratio:
                best_gain_ratio = gain_ratio_val
                best_feature = feature
    return best_feature, best_split_value


# 构建决策树
def c45(d):
    labels = d.iloc[:, -1]

    # 递归临界条件：1. label 的值全部相同; 2. 没有多余特征可供选择，直接使用标签中多数标记
    if len(labels.unique()) == 1:
        return labels.iloc[0]

    if len(d.columns) == 1:
        return labels.value_counts().idxmax()

    best_feature_name, best_split_value = best_feature(d)
    tree = {}

    if best_split_value is not None:
        data_left = d[d[best_feature_name] <= best_split_value]
        data_right = d[d[best_feature_name] > best_split_value]

        tree[best_feature_name] = {}
        tree[best_feature_name][f'<= {best_split_value}'] = c45(data_left.drop(columns=[best_feature_name]))
        tree[best_feature_name][f'> {best_split_value}'] = c45(data_right.drop(columns=[best_feature_name]))
    else:
        # 对于 best_feature 不是连续型特征的处理
        tree[best_feature_name] = {}
        for value in d[best_feature_name].unique():
            subtree = c45(d[d[best_feature_name] == value].drop(columns=[best_feature_name]))
            tree[best_feature_name][value] = subtree

    return tree


# 决策树可视化
def visualize_tree(tree, dot=None, parent=None, edge=None):
    if dot is None:
        dot = Digraph()
        dot.node(name='root', label=list(tree.keys())[0])
        parent = 'root'

    for edge_value, subtree in tree[list(tree.keys())[0]].items():
        node_name = str(id(subtree))
        if isinstance(subtree, dict):
            node_label = list(subtree.keys())[0]
            dot.node(name=node_name, label=node_label)
            dot.edge(parent, node_name, label=str(edge_value))
            visualize_tree(subtree, dot, node_name, edge_value)
        else:
            dot.node(name=node_name, label=str(subtree), shape='box')
            dot.edge(parent, node_name, label=str(edge_value))

    return dot


# 示例数据集
data = pd.DataFrame({
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny',
                'Overcast', 'Overcast', 'Rain'],
    'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71],
    'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80],
    'Windy': [False, True, False, False, False, True, True, False, False, False, True, True, False, True],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
})

# 构建决策树
decision_tree = c45(data)

# 可视化决策树
dot = visualize_tree(decision_tree)
dot.render('c45_decision_tree', format='png', cleanup=True)  # 保存为PNG图片
dot.view()  # 打开图片查看
```



## 1.4 CART

CART（Classification And Regression Tree） 是以基尼系数为准则选择最优划分属性，可用于分类和回归。

不同于 ID3 和 C4.5 算法，CART 是一颗二叉树，且可以用于分类和回归，实际应用中,CART 使用较多。

Gini Index 又称为 Gini Impurity，基尼指数量化了一个节点中样本的不纯度，基尼指数越低，表示节点的纯度越高，假设一个节点中的样本共有 K 种不同的类别，每种类别的样本占比为 $p_i, i=1, 2,…,K$，则：
$$
Gini=1-\sum_{i=1}^K{p_i^2}
$$

- $Gini = 0$：节点完全纯净，所有样本同属于同一类别
- $Gini \rightarrow 1$: 样本中类别分布混杂，各类别的比例比较接近

Gini Index 在决策树中的应用：每次分裂时，选择<u>基尼指数最小化</u>的划分点作为节点的分裂点，从而增加子节点的纯度





> [!tip]
>
> 分类（classification）和回归（Regression）的区别：
>
> |          | classification                               | Regression                                        |
> | -------- | -------------------------------------------- | ------------------------------------------------- |
> | 目标类型 | 离散类别                                     | 连续数值                                          |
> | 评估指标 | accuracy,precision,recall,F1                 | MSE,RMSE(均方根误差),MAE(平均绝对误差)            |
> | 模型输出 | 概率分布，然后根据阈值确定最终类别           | 预测值                                            |
> | 损失函数 | Cross-Entropy Loss                           | Mean Squared Error Loss                           |
> | 应用场景 | 文本分类,图像识别,语音识别                   | 股价预测,销售额预测,温度预测                      |
> | 常见模型 | 逻辑回归,支持向量机,决策树,随机森林,神经网络 | 线性回归,岭回归,LASSO回归,决策树回归,随机森林回归 |



代码实现

```python
import pandas as pd
# import numpy as np
from graphviz import Digraph
# from sklearn.preprocessing import OrdinalEncoder

# 计算基尼指数
def gini(d):
    labels = d.iloc[:, -1]
    # 得到每个标签及其出现次数
    label_counts = labels.value_counts()
    total = len(labels)
    gini_index = 1 - sum((count / total) ** 2 for count in label_counts)
    return gini_index


# 计算分裂后的加权基尼指数
def weighted_gini(data_left, data_right):
    total = len(data_left) + len(data_right)
    gini_left = gini(data_left)
    gini_right = gini(data_right)
    weighted_gini_index = (len(data_left) / total) * gini_left + (len(data_right) / total) * gini_right
    return weighted_gini_index


# 寻找最佳分裂点
def best_split(d):
    # 获取特征空间的特证名
    features = d.columns[:-1]
    # float('inf') 代表浮点数类型的正无穷大
    best_gini = float('inf')
    best_split_feature = None
    best_split_value = None

    for feature in features:
        unique_values = d[feature].unique()
        for value in unique_values:
            data_left = d[d[feature] <= value]
            data_right = d[d[feature] > value]
            gini_index = weighted_gini(data_left, data_right)
            if gini_index < best_gini:
                best_gini = gini_index
                best_split_feature = feature
                best_split_value = value

    return best_split_feature, best_split_value


# 构建CART决策树
def cart(d, depth=0, max_depth=None):
    labels = d.iloc[:, -1]

    # 停止条件：所有样本属于同一类或达到最大深度
    if len(labels.unique()) == 1 or (max_depth is not None and depth >= max_depth):
        # mode() 用于返回出现频率高的序列，dropna 参数可用于表示在计算前是否去除 NaN 缺失值，默认为 True
        return labels.mode()[0]

    best_feature, best_value = best_split(d)

    if best_feature is None:
        return labels.mode()[0]

    tree = {}
    tree[f"{best_feature} <= {best_value}"] = {}

    data_left = d[d[best_feature] <= best_value]
    data_right = d[d[best_feature] > best_value]

    tree[f"{best_feature} <= {best_value}"]['Yes'] = cart(data_left, depth + 1, max_depth)
    tree[f"{best_feature} <= {best_value}"]['No'] = cart(data_right, depth + 1, max_depth)

    return tree


# 决策树可视化
def visualize_tree(tree, dot=None, parent=None, edge=None):
    if dot is None:
        dot = Digraph()
        dot.node(name='root', label=list(tree.keys())[0])
        parent = 'root'

    for edge_value, subtree in tree[list(tree.keys())[0]].items():
        node_name = str(id(subtree))
        if isinstance(subtree, dict):
            node_label = list(subtree.keys())[0]
            dot.node(name=node_name, label=node_label)
            dot.edge(parent, node_name, label=str(edge_value))
            visualize_tree(subtree, dot, node_name, edge_value)
        else:
            dot.node(name=node_name, label=str(subtree), shape='box')
            dot.edge(parent, node_name, label=str(edge_value))

    return dot

if __name__ == '__main__':
    # 示例数据集
    data = pd.DataFrame({
        # 'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny',
        #             'Overcast', 'Overcast', 'Rain'],
        'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71],
        'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80],
        # 'Windy': [False, True, False, False, False, True, True, False, False, False, True, True, False, True],
        'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
    })

    # data.iloc[:, :1] = OrdinalEncoder().fit_transform(data.iloc[:, :1])
    # print(data)

    # 构建CART决策树
    cart_tree = cart(data)

    # 可视化决策树
    dot = visualize_tree(cart_tree)
    dot.render('cart_decision_tree', format='png', cleanup=True)  # 保存为PNG图片
    dot.view()  # 打开图片查看
```

> [!warning]
>
> 上述代码主要采用基尼指数选择最优的分裂特征，表示这是一颗 CART 决策树，如果想改造为一颗 CART 回归树，需要使用 MSE 最为特征选择指标



# 2 随机森林

随机森林（random forests or random decision forests）是一种用于分类、回归等任务的集成学习算法，主要是由多颗决策树构成，每颗决策树是一个独立的分类器 / 回归器。

随机森林的集成方法是 Bagging（Bootstrap Aggregating），对训练集进行有放回抽样，生成若干个不同的子数据集，然后每个子数据集训练一颗决策树，最终的预测结果通过分类或回归得到。

:balloon:**组合分类器的优点**

1. 通过整合各个模型的分类结果，可以得到更加合理的决策边界，减少整体错误，提升模型精度

2. 过大的数据集可以直接将其划分为多个子集，过小的数据集可通过自助采样的方式扩展得到新的数据集

3. 决策边界过于复杂时，可以构建多个线性分类器，然后将其集成

   <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240831153100653.png" alt="image-20240831153100653" style="zoom:50%;" />

4. 可处理多源异构数据



> [!note]
>
> 随机森林的随机主要体现在那几个方面？
>
> 

# 3 Boosting

集成算法有两类：Bagging 和 Boosting，其中 Bagging 和 Boosting 算法差别如下：

|                           Bagging                            |                           Boosting                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src='https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/Snipaste_2024-08-31_19-33-53.png'> | <img src='https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/Snipaste_2024-08-31_19-34-06.png'> |
|                 并行训练，每个训练器相互独立                 |            串行训练，新的学习器依赖于上一个训练器            |
|             $Bagging=\frac{1}{N}\sum_{i=1}^Ns_i$             |  $Boosting=\sum_{i=1}^N{w_is_i}$，表现越好，学习器权重越大   |

Bagging 和 Boosting 的选择策略：<u>若单个学习器效果较差，那么选择 Boosting 算法；若多个学习器会过拟合，那么 Bagging 算法效果较好</u>。

对于 Boosting 算法而言，需要解决如下几个问题：

1. 如何计算学习误差率 $e$
2. 如何得到若学习器权重系数 $\alpha$
3. 如何更新样本权重 $w$
4. 使用何种组合策略



<img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240902083845175.png" alt="image-20240902083845175" style="zoom:50%;" />



## 3.1 Adaboost

> [!tip]
>
> 以下推论都是基于 Ababoost 处理的是二分类问题，输出结果为 $\{-1, 1\}$



1. 假设训练样本集为: $T= \{ (\boldsymbol{x_1}, y_1), (\boldsymbol{x_2}, y_2), \cdots, (\boldsymbol{x_m}, y_m)\}$, 初始化所有样本的困难度 $w_i=\frac{1}{N}$

   > [!note]
   >
   > 使得所有样本的困难度相加为 1 是为了确保每次迭代中权重的归一化处理，防止精度溢出

2. 计算加权误差率，第 k 个弱分类器 $G_k(x)$ 在训练集上的加权误差如下 
   $$
   e_k=P\left(G_k\left(x_i\right) \neq y_i\right)=\sum_{i=1}^m w_{k,i} I\left(G_k\left(x_i\right) \neq y_i\right)
   $$

   > [!tip]
   >
   > $P$ —— 概率函数，$I$ —— 指示函数

3. 接着我们计算弱分类器权重系数，对于二分类问题，第 k 个分类器 $G_k(x)$ 的权重系数为
   $$
   \alpha_k = \frac{1}{2}log(\frac{1-e_k}{e_k})
   $$
   <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240905105851555.png" alt="image-20240905105851555" style="zoom:33%;" />

   >[!note]
   >
   >函数在 [0, 1] 为递减函数，所以加权误差率越大，对应的弱分类器权重越小

4. 更新样本的困难度，其中 $w_{k,i}$ 代表在第 k 个分类器处理第 i 个样本时困难度（权值），$y_i$ 代表第 i 个样本的真值
   $$
   w_{k+1, i}=\frac{w_{k,i}}{Z_k}e^{-\alpha_k y_iG_k(x_i)}
   $$
   

   $Z_k$ 是规范化因子，用于维持样本的困难度之和不变，其中 $Z_k=\sum_{i=1}^mw_{k,i}e^{-\alpha_ky_iG(x_i)}$

   > [!tip]
   >
   > 从 $w_{k+1, i}$ 的等式中可以看到，如果第 i 个样本分类错误，那么 $y_iG_k(x_i) < 0$，$e$ 上的指数则为正，则该类的权重会提升，反之则反

5. 组合各个弱分类器，得到强分类器
   $$
   \begin{flalign}
   & f(x)=\sum_{k=1}^K{\alpha_kG_k(x)} \\
   & G(x)=sign(f(x))
   \end{flalign}
   $$

将上述数学过程简单转换为代码实现

```python
import numpy as np


def _g_1(x):
    """
        用于计算分类器 1 得出的结果
    """
    if x < 2.5:
        return 1
    elif x > 2.5:
        return -1


def _g_2(x):
    """
        用于计算分类器 2 得出的结果
    """
    if x < 8.5:
        return 1
    elif x > 8.5:
        return -1


def _w(x):
    """
        用于计算分类器权值
    """
    return np.log((1 - x) / x) / 2


def main():
    X = [i for i in range(0, 10)]
    y = [1, 1, 1, -1, -1, -1, 1, 1, 1, -1]
    D = [0.1 for _ in range(10)]
    # 初始化循环轮数
    num = 1
    # 初始化分类器权重系数
    a = 0
    for _ in range(num):
        # 初始化加权误差率
        e = 0
        for i in range(10):
            # 计算加权误差率
            e += D[i] * (_g_1(X[i]) != y[i])
        # 得到分类器权重系数
        a = _w(e)

        # 计算规范因子 Z
        Z = 0
        for i in range(10):
            Z += D[i] * np.exp(-1 * a * y[i] * _g_1(X[i]))

        # 更新样本的困难度
        for i in range(10):
            D[i] = D[i] / Z * np.exp(-1 * a * y[i] * _g_1(X[i]))
    print(a)
    

if __name__ == '__main__':
    main()
```




## 3.2 GBDT

GBDT（Gradient Boosting Decision Tree）是梯度提升树，可用作分类或者回归。

> [!tip]
>
> - 如果 GBDT 被用作回归树，那么划分的标准是 MSE
> - 如果 GBDT 被用作分类树，那么划分的标准是 Gini（即以 CART 树作为基学习器）

GBDT 的核心：多次迭代，每次迭代生成一个新的弱学习器，这个弱学习器是对上一轮**学习器的残差进行拟合**，从而降低训练数据的损失函数值。

以下以年龄作为示例：

![Snipaste_2024-09-06_00-10-47](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/Snipaste_2024-09-06_00-10-47.png)

对于 GBDT 算法，模型描述为：
$$
\begin{flalign}
& F_m(x)=\sum_{m=1}^M T\left(x ; \theta_m\right) \\
& F_m(x) = F_{m-1}(x)+\lambda T(x;\theta_m)
\end{flalign}
$$

> [!note]
>
> 其中，F 代表当前的模型，T 代表弱分类器，上述公式代表当迭代到第 m 轮时，模型的预测函数 $F_m(x)$ 由多个弱学习器 $T(x;\theta_m)$ 组成，$x$ 为输入特征，$\theta_m$ 代表该学习器的要学习的残差

对于弱分类器，其损失函数为：
$$
\hat{\theta}_m=\underset{\theta_m}{\arg \min } \sum_{i=1}^N L\left(y_i, F_{m-1}\left(x_i\right)+T\left(x_i ; \theta_m\right)\right)
$$
而对于 L 的选择，可以由多种，包括 平方损失函数，0-1 损失函数，对数损失函数，若我们选择的是平方损失函数，那么这个 $\theta$ 即我们上面提到的残差。

**代码简单实现**

```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import make_regression
from sklearn.metrics import mean_squared_error
import numpy as np
import matplotlib.pyplot as plt


class GBDT:
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):
        self.pred = None  # 预测结果
        self.n_estimators = n_estimators  # 弱学习器的数量
        self.learning_rate = learning_rate  # 学习率，用来控制修正步长，防止过拟合
        self.max_depth = max_depth  # 决策树的最大深度
        self.trees = []  # 存储训练好的决策树

    def fit(self, X, y):
        self.pred = np.zeros(X.shape[0])
        # 迭代训练每一棵树
        for _ in range(self.n_estimators):
            # 计算残差
            residuals = y - self.pred

            # 训练决策树
            tree = DecisionTreeRegressor(max_depth=self.max_depth)
            tree.fit(X, residuals)

            # 更新预测值
            self.pred += self.learning_rate * tree.predict(X)

            self.trees.append(tree)

    def predict(self, X):
        pred = np.zeros(X.shape[0])
        for tree in self.trees:
            pred += self.learning_rate * tree.predict(X)
        return pred


def main():
    X, y = make_regression(n_samples=100, n_features=20, noise=0.1)
    # c=y 代表将颜色与y值进行映射
    plt.scatter(X[:, 0], X[:, 1], c=y)
    plt.show()

    gbdt = GBDT(n_estimators=10, learning_rate=0.5, max_depth=3)
    gbdt.fit(X, y)
    predictions = gbdt.predict(X)
    # 打印预测值结果
    print(predictions)
    # 得到均方误差
    mse = mean_squared_error(y, predictions)
    print(mse)


if __name__ == '__main__':
    main()
```



## 3.3 XGBoost

XGBoost（eXtreme Gradient Boosting） 极致梯度提升算法，是一种基于 GBDT 的改进算法，下面我们将以回归树为基学习器的 XGBoost 作为案例进行讲解。



> [!tip]
>
> 什么是范数（norm）?
>
> 在线性代数中，范数是指空间中所有向量总的长度，衡量这些向量有不同的方法，最常见的包括 L0 范数、L1 范数和 L2 范数
>
> - L0 范数（实际上这不能被称为是一个范数）：向量中非 0 元素的个数
>
> - P 范数：$\|x\|_p=\left(|x 1|^p+|x 2|^p+\ldots+|x n|^p\right)^{1 / p}$，当 P 取特定值时的，范数如下
>
>   - L1 范数：向量中各个元素的绝对值之和
>     $$
>     \|x\|_1=|x_1|+|x_2|+\ldots+|x_n|
>     $$
>
>   - L2 范数：向量中各元素的平方和然后求平方根
>     $$
>     \|x\|_2=\left(|x_1|^2+|x_2|^2+\ldots+|x_n|^2\right)^{1 / 2}
>     $$
>
>   - $\infty$-范数：向量中各元素绝对值的最大值
>     $$
>     \|x\|_{\infty}=\max (|x_1|,|x_2|, \ldots,|x_n|)
>     $$
>
> 其中 L1 范数也被称为 "稀疏规则算法"（Lasso regularization），又称曼哈顿距离，常用于计算目标值与估计值之间的差值的绝对值的总和：$S=\sum_{i=1}^n|y_i-f(x_i)|$



### 3.3.1 回归树







# 4 朴素贝叶斯

在条件概率中，我们知道有如下式子:
$$
P(Y\ |\ X)=\frac{P(Y)P(X\ |\ Y)}{P(X)}
$$
上述公式中，$P(Y|X)$ 是条件概率，我们可以将 $X$ 看一个 Observation, $Y$ 为 Hypothesis（假设），$P(Y\ |\ X)$ 为 $Y$ 的后验概率（Posterior probability），$P(Y)$ 为 $Y$ 的先验概率（Prior probability）

**什么是条件独立（conditional independence）?**

>In probability theory, **conditional independence** describes situations wherein an observation is irrelevant or redundant when evaluating the certainty of a hypothesis

假设 X，Y 变量在给定第三个随机变量 Z 的情况下条件独立，那么他们在给定 Z 的情况下条件概率分布相互独立
$$
\begin{aligned}
P(X, Y|Z) &=P(X|Z)P(Y|Z)
\end{aligned}
$$
对于朴素贝叶斯，它假设各个特征变量之间相互独立，也就是说对于特征变量 $\boldsymbol{X}={x_1, x_2, \cdots, x_n}$ 其推测出的标签为 $y$（这里的 $y$ 即为假设） ，我们要得便是概率值最大时的类是哪一个。
$$
\begin{align}
h(x) &= \underset{k}{\mathrm{argmax}} \ P(k \mid x) \tag{1} \\
&= \underset{k}{\mathrm{argmax}} \ P(k \mid x_1, x_2, \dots, x_p) \tag{2} \\
&= \underset{k}{\mathrm{argmax}} \ \frac{P(k) P(x_1, x_2, \dots, x_p \mid k)}{P(x_1, x_2, \dots, x_p)} \tag{3} \\
&= \underset{k}{\mathrm{argmax}} \ P(k) P(x_1, x_2, \dots, x_p \mid k) \tag{4} \\
&= \underset{k}{\mathrm{argmax}} \ P(k) P(x_1 \mid k) P(x_2 \mid k) \dots P(x_p \mid k) \tag{5} \\
&= \underset{k}{\mathrm{argmax}} \ P(k) \prod_{i=1}^{p} P(x_i \mid k) \tag{6}
\end{align}
$$

> [!note]
>
> 其中 (3) -> (4) 是由于 $P(x_1, x_2, \cdots, x_n) = P(x_1)P(x_2)\cdots P(x_n)$ ，其值确定，不会影响选取概率最大的 k 的结果



