[toc]



# 1 å¼•å…¥

Machine Learning(ML): acquiring **skill** with experience accumulated/computed from **data**

```mermaid
graph LR
    A("data") --> B("ML")
    B --> C("skill")
    style A stroke-dasharray: 5 5
    style B stroke:#333,stroke-width:3px
    style C stroke-dasharray: 5 5
```

> [!note]
>
> common application
>
> 1. speech recognition
> 2. image recognition
> 3. playing go
>



æœºå™¨å­¦ä¹ åˆ†ç±»ï¼š

1. supervised learning
2. Semi-supervised learning
3. unsupervised learning



> [!tip]
>
> åœ¨æœºå™¨å­¦ä¹ æˆ–æ·±åº¦å­¦ä¹ ä¸­çš„å•è¯ç¼©å†™ï¼š
>
> - SOTA(state of the art): æœ€å…ˆè¿›çš„

## 1.1 self-supervised learning

self-supervised learningï¼šè‡ªç›‘ç£å­¦ä¹ ï¼Œé¢„å…ˆè¿›è¡Œä¸€äº›è®­ç»ƒï¼ˆpre-trainï¼‰ï¼Œç„¶åå†æ¥å¤„ç†æ•°æ®

:telescope:ä¾‹å¦‚ï¼šä¸‹å›¾ä¼šå°†å›¾ç‰‡è¿›è¡Œåè½¬å’Œè°ƒè‰²ï¼Œç„¶åè®©æ¨¡å‹è‡ªè¡Œåˆ¤æ–­å›¾ç‰‡ä¸­ä¸»ä½“æ˜¯å¦æ˜¯åŒä¸€ä¸ªï¼Œé€šè¿‡é¢„è®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥è®©æ¨¡å‹å…ˆæ‰“å¥½ "åŸºæœ¬åŠŸ"ï¼Œç„¶åå†å¯¹æ•°æ®é›†è¿›è¡Œå¤„ç†ï¼Œä»å¼€å‘é€šç”¨ç›®æ ‡çŸ¥è¯†ï¼ˆdevelop general purpose knowledgeï¼‰

<img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240922154406252.png" alt="image-20240922154406252" style="zoom: 67%;" />

å½“å­¦ä¹ è¿™äº› "åŸºæœ¬åŠŸ" åï¼Œè¯¥æ¨¡å‹å¤„ç†ä¸‹æ¸¸ä»»åŠ¡ï¼ˆdownstream tasksï¼‰å°±æ¯”è¾ƒå¥½å¤„ç†

<img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240922155839143.png" alt="image-20240922155839143" style="zoom:50%;" />



pre-trained model å’Œ downstream tasks çš„å…³ç³»ç±»ä¼¼äºæ“ä½œç³»ç»Ÿå’Œåº”ç”¨ä¹‹é—´çš„å…³ç³»ï¼Œä¸‹æ¸¸ä»»åŠ¡åªéœ€è¦åœ¨ pre-trained model ä¸Šè¿›è¡Œå®ç°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒåº•å±‚çš„äº¤äº’é€»è¾‘ï¼Œå› æ­¤ pre-trained model åˆè¢«ç§°ä¸º ==foundation model==

> [!note]
>
> åœ¨ foundation model ä¸­ï¼Œæœ€ä¸ºäººæ‰€ç†ŸçŸ¥çš„ä¾¿æ˜¯ ==BERT==



## 1.2 Generative Adversarial Network

ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenerative Adversarial Networksï¼Œ GANsï¼‰ï¼šGan ä¸»è¦æ˜¯ç”± ==ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰== å’Œ==åˆ¤åˆ«å™¨ï¼ˆDiscriminatorï¼‰== ç»„æˆï¼Œå…¶ä¸­ç”Ÿæˆå™¨ä¸æ–­ä¼˜åŒ–è‡ªå·±ç”Ÿäº§çš„æ•°æ®è®©åˆ¤åˆ«å™¨åˆ¤æ–­ä¸å‡ºæ¥ï¼Œåˆ¤åˆ«å™¨ä¹Ÿè¦ä¼˜åŒ–è‡ªå·±è®©è‡ªå·±åˆ¤åˆ«åœ°æ›´åŠ å‡†ç¡®ï¼Œä¸¤è€…å½¢æˆå¯¹æŠ—å…³ç³»ã€‚

<img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240922163822583.png" alt="image-20240922163822583" style="zoom:50%;" />

æ¨èè®ºæ–‡

| æ–¹å‘                                   | è®ºæ–‡                                                         |
| -------------------------------------- | ------------------------------------------------------------ |
| Unsupervised Abstractive Summarization | https://arxiv.org/abs/1810.02851                             |
| Unsupervised Translation               | https://arxiv.org/abs/1710.04087<br />https://arxiv.org/abs/1710.11041 |
| Unsupervised ASR                       | https://arxiv.org/abs/1804.00316<br />https://arxiv.org/abs/1812.09323<br />https://arxiv.org/abs/1904.04100<br />https://arxiv.org/abs/2105.11084 |



## 1.3 Reinforcement Learning

application scenarios: *it is challenging to label data in some tasks*.

> [!tip]
>
> when we can't know the results are good or not, use **RL**

 

## 1.4 Advanced Topic

1. Anomaly Detection
2. Explainable AI 
3. Model Attack
4. Domain Adaptation 
5. Network Compression
6. Life-long Learning
7. Meta learning



# 2 æœºå™¨å­¦ä¹ åŸºæœ¬æ¦‚å¿µ   

**different types of functions** 

- **Regression**: The function outputs a scalar.

  > [!tip]
  >
  > example: predict PM2.5

- **Classification**: Given options(classes), the function outputs the correct one.

  > [!tip]
  >
  > example: spam filtering

- **Structured Learning**: Create something with structure(image, document)



æœºå™¨å­¦ä¹ å»ºæ¨¡çš„åŸºæœ¬æµç¨‹

1. find a **function** with unknown parameters, e.g. $y=wx+b$

   > [!tip]
   >
   > Here, function == model

2. define **loss** from training data, $L(b, w)$

   <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240923101459616.png" alt="image-20240923101459616" style="zoom:50%;" />

3. optimization: $w^*, b^*=arg\ \mathop{min}\limits_{w, b} L$

   Gradient Descent:

   - (randomly) pick a initial value $w^0$, $b^0$

   - compute $\begin{equation}\frac{\partial L}{\partial w}|_{w=w^0}\end{equation}$, $\begin{equation}w^1\leftarrow w^0-\eta\frac{\partial L}{\partial w}|_{w=w^0}\end{equation}$, $\begin{equation}b^{1}\leftarrow b^{0}-\eta\frac{\partial L}{\partial b}|_{{w=w^{0},b=b^{0}}}\end{equation}$

     <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240923101716562.png" alt="image-20240923101716562" style="zoom:50%;" />

   - update $w$ iteratively

   > [!warning]
   >
   > 1. åœ¨æ¢¯åº¦ä¸‹é™çš„è¿‡ç¨‹ä¸­ï¼Œ$step=\eta\frac{\partial L}{\partial w}$ï¼Œå…¶ä¸­ $\eta$ ä¸º learning rate, $\frac{\partial L}{\partial w}$ ä¸ºä»»æ„ç‚¹åœ¨æŸå¤„çš„å¾®åˆ†ï¼Œå½“è¾¾åˆ° minima ç‚¹æ—¶ï¼ˆ==local minima== or ==global minima==ï¼‰ï¼Œ$\frac{\partial L}{\partial w}=0$ï¼Œç§»åŠ¨å°†ä¼šåœä¸‹
   > 2. æˆ‘ä»¬éƒ½çŸ¥é“æŸå¤±å‡½æ•°æ— è®ºæ˜¯ä½¿ç”¨ L1-norm è¿˜æ˜¯ L2-norm, è®¡ç®—çš„ç»“æœéƒ½æ˜¯å¤§äºç­‰äº 0 çš„ï¼Œä¸ºä»€ä¹ˆåœ¨è¿™é‡Œå›¾ä¸­æ˜¾ç¤ºçš„ Loss å¯ä»¥å°äº 0ï¼Œ åŸå› æ˜¯ Loss å‡½æ•°åªæ˜¯ç”¨åˆ°äº† L1-norm æˆ– L2-normï¼Œä½†æ˜¯å…·ä½“çš„å‡½æ•°è¡¨è¾¾å¼å¯èƒ½å¢åŠ äº†æƒ©ç½šé¡¹ï¼Œä¾‹å¦‚ `-1000`ï¼Œæ‰€ä»¥ä¼šå‡ºç° Loss å‡½æ•°çš„å€¼å°äº 0 çš„æƒ…å†µå‘ç”Ÿ
   > 3. åœ¨æ¢¯åº¦ä¸‹é™çš„ç®—æ³•ï¼Œä»å›¾ä¸­å¯ä»¥çœ‹å‡ºä¼šå‡ºç°å±€éƒ¨æœ€å°çš„æƒ…å†µï¼Œä½†æ˜¯å®é™…ä¸Šè¿™å¹¶ä¸æ˜¯æ¢¯åº¦ä¸‹é™çš„ä¸»è¦é—®é¢˜ï¼Œæ¢¯åº¦ä¸‹é™çœŸæ­£çš„ç—›ç‚¹åœ¨åé¢ä¼šæåˆ°



:red_circle:**æ€»ç»“**

ä¸€èˆ¬è€Œè¨€ï¼Œæœºå™¨å­¦ä¹ çš„è¿‡ç¨‹å¦‚ä¸‹ï¼š

1. æ”¶é›†è®­ç»ƒæ•°æ®: $\left\{\left(\boldsymbol{x^1}, y^1\right),\left(\boldsymbol{x^2}, y^2\right), \ldots,\left(\boldsymbol{x^N}, y^N\right)\right\}$

   > [!tip]
   >
   > å…¶ä¸­ $x$ ä»£è¡¨è®­ç»ƒæ•°æ®çš„è¾“å…¥ï¼Œå³æ ·æœ¬ï¼Œ$y$ ä»£è¡¨è®­ç»ƒæ•°æ®çš„è¾“å‡ºï¼Œå³çœŸå€¼

2. è¿›è¡Œè®­ç»ƒ

   ```mermaid
   graph LR
   A("Step 1:<br>function with unknown<br>$$y=f_{\theta}(x)$$")
   B("Step 2:<br>loss from training data<br>$$L(\boldsymbol{\theta})$$")
   C("Step 3:<br>optimization<br>$$\boldsymbol{\theta}^*=\arg\  \min _{\boldsymbol{\theta}} L$$")
   A --> B
   B --> C
   ```

   > [!note]
   >
   > å…¶ä¸­è®­ç»ƒé›†éœ€è¦è¿›ä¸€æ­¥æ‹†åˆ†å‡ºä¸€ä¸ªéªŒè¯é›†ï¼ˆvalidation setï¼‰ï¼Œå…¶ä»–çš„æ•°æ®é›†ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œä¸ºäº†æ›´åŠ å……åˆ†åœ°åˆ©ç”¨è®­ç»ƒé›†ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šåš k æŠ˜äº¤å‰éªŒè¯ï¼ˆk-fold Cross-Validation)

3. å°†è®­ç»ƒå¥½çš„æ¨¡å‹è¿ç”¨åˆ°æµ‹è¯•é›†ä¸­ï¼Œåœ¨ä¸€èˆ¬çš„æ•°æ®æŒ–æ˜å’Œé¢„æµ‹çš„å¹³å°ï¼Œä¾‹å¦‚ Kaggleï¼Œä¸€èˆ¬åªæä¾› public testing setï¼Œè€Œå¯¹ private testing set æ˜¯éšè—èµ·æ¥é¿å…ç”¨æˆ·è®­ç»ƒæ¨¡å‹åªæ˜¯æ°å¥½åœ¨ public testing set ä¸­çš„æ•ˆæœå¥½ï¼Œè€Œåœ¨æœªçŸ¥æ•°æ®é›†ä¸Šè¡¨ç°ä¸€èˆ¬

   Testing data: $\left\{\left(\boldsymbol{x^{N+1}}, \hat{y}^{N+1}\right),\left(\boldsymbol{x^{N+2}}, \hat{y}^{N+2}\right), \ldots,\left(\boldsymbol{x}^{N+M}, \hat{y}^{N+M}\right)\right\}$

   > [!tip]
   >
   > ä¸€èˆ¬è€Œè¨€ï¼Œæˆ‘ä»¬è§„å®š $y$ ä»£è¡¨çœŸå€¼ï¼ˆground truthï¼‰ï¼Œè€Œ $\hat{y}$ ä»£è¡¨é¢„æµ‹å€¼ï¼ˆpredict valueï¼‰





**å¦‚ä½•é’ˆå¯¹ loss å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Ÿ**

æˆ‘ä»¬å¯ä»¥æŒ‰ç…§ä¸‹é¢çš„è¿™ä¸ªæµç¨‹è¿›è¡Œå¤„ç†

![image-20241003202510975](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241003202510975.png)

ğŸ‘¨â€ğŸ«å¯¹å›¾ç¤ºè¿›è¡Œéƒ¨åˆ†è§£é‡Šå’Œè¡¥å……

1. model bias vs optimization issue

   - model bias å³æ¨¡å‹åå·®ï¼Œæ˜¯ç”±äº<u>å»ºæ¨¡è¿‡äºç®€å•ï¼Œä¸å¤Ÿæœ‰å¼¹æ€§</u>å¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒæ—¶ï¼Œæ— è®ºæ€ä¹ˆæ”¹å˜æ¨¡å‹å‚æ•°ï¼Œè¯¯å·®ä»ç„¶å¾ˆå¤§
   - optimization issue å³ä¼˜åŒ–é—®é¢˜ï¼Œç”±äºåœ¨ä½¿ç”¨ GB ç®—æ³•å­˜åœ¨ä¸€ä¸ª local minimaï¼ˆå±€éƒ¨æå°å€¼ï¼‰ çš„é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯èƒ½æ‰¾åˆ°çš„å‚æ•°ï¼ˆæˆ–å‚æ•°å‘é‡ï¼‰å¹¶ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è§£ï¼Œå¯¼è‡´è¯¯å·®å¾ˆå¤§

   > [!tip]
   >
   > å¦‚ä½•åˆ¤æ–­æ˜¯ model bias è¿˜æ˜¯ optimization issue?
   >
   > å¤šæ·»åŠ å‡ ä¸ªå±‚æ•°æ›´å°‘çš„ model è¿›è¡Œå¯¹æ¯”ï¼Œå¦‚æœæ·±å±‚ç¥ç»ç½‘ç»œæ¯”æµ…å±‚ç¥ç»ç½‘ç»œè¯¯å·®æ›´å¤§ï¼Œé‚£ä¹ˆæˆ‘ä»¬è€ƒè™‘æ­¤æ—¶æ·±å±‚åœ£ç»ç½‘ç»œæ²¡æœ‰å‚æ•°ä¼˜åŒ–æ²¡æœ‰åšå¥½

2. overfitting åªæœ‰å½“æˆ‘ä»¬åœ¨åšæµ‹è¯•æ—¶æ‰èƒ½åˆ¤æ–­è¯¯å·®æ˜¯ç”±è¿‡æ‹Ÿåˆäº§ç”Ÿçš„







# 3 æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ

:bulb:**å‰ç½®çŸ¥è¯†: sigmoid å‡½æ•°**

å¦‚ä¸‹å³ä¸º sigmoid function è¡¨è¾¾å¼ï¼š
$$
y=c\frac{1}{1+e^{-(b+wx)}}=\sigma(b+wx)
$$
<img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240923131941279.png" alt="image-20240923131941279" style="zoom: 33%;" />

å½“æˆ‘ä»¬å°† sigmoid å‡½æ•°åˆ†æ®µç¡¬ç›´åŒ–æ—¶ï¼Œæ­¤æ—¶çš„ sigmoid å‡½æ•°åˆç§°ä¸º hard sigmoidï¼Œç›¸å¯¹è€Œè¨€ï¼Œæˆ‘ä»¬å¯ä»¥ç§°ä¸Šè¿°çš„ sigmoid å‡½æ•°ä¸º soft sigmoid

æœ€ç®€åŒ–çš„ sigmoid å‡½æ•°ä¸º $y=\frac{1}{1+e^{-x}}=\sigma(x)$ï¼Œå…¶å¯¼å‡½æ•° $y^{\prime}=\frac{e^{-x}}{(1+e^{-x})^2}=\sigma(x)\cdot (1-\sigma(x))$

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ python ä»£ç å®ç°ï¼š

```python
import matplotlib.pyplot as plt
import numpy as np
def sigmoid(x):
  return 1 / (1 + np.exp(-x))


def sigmoid_plot(x):
  y_1 = sigmoid(x)
  y_2 = sigmoid(x) * (1 - sigmoid(x))
  plt.plot(x, y_1, 'b', label='sigmoid')
  plt.plot(x, y_2, 'r', label='sigmoid derivative')
  plt.legend()
  plt.show()


if __name__ == '__main__':
  x = np.arange(-4, 3, 0.2)
  sigmoid_plot(x)
```

![image-20241003155140804](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241003155140804.png)





:question: é—®é¢˜å¼•å…¥ï¼š<u>ç°åœ¨æˆ‘è¦æ ¹æ®æ•°æ®é›†åˆ›å»ºä¸€ä¸ªæ¨¡å‹ç”¨æ¥é¢„æµ‹ä¸€ä¸ªé¢‘é“çš„è§‚çœ‹äººæ•°ï¼Œè¯·é—®è¯¥å¦‚ä½•æ„å»ºè¿™ä¸ªæ¨¡å‹ï¼Ÿ</u>

å¦‚æœæˆ‘ä»¬ä½¿ç”¨çº¿æ€§æ¨¡å‹ï¼Œç”±äºçº¿æ€§æ¨¡å‹è¿‡äºç®€å•ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è‚¯å®šæ˜¯ä¸é€‚ç”¨çš„ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæ›´åŠ å¤æ‚çš„æ¨¡å‹

> [!tip]
>
> åœ¨å¤§éƒ¨åˆ†åœºæ™¯ä¸­ï¼Œçº¿æ€§æ¨¡å‹å…·æœ‰æ¯”è¾ƒå¤§çš„æ¨¡å‹åå·®ï¼ˆmodel biasï¼‰ï¼Œæ¨¡å‹åå·®æè¿°äº†æ¨¡å‹é¢„æµ‹çš„å¹³å‡è¯¯å·®ï¼Œç”¨ç¬¦å·è¡¨ç¤ºä¸º $\frac{1}{n}\sum_{i=1}^{n}{|y-\hat{y}|}$



å¯¹äºä¸‹å›¾ï¼Œå‡è®¾æˆ‘ä»¬éœ€è¦æ„å»ºçš„å‡½æ•°ä¸ºçº¢è‰²æ›²çº¿éƒ¨åˆ†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ç†è§£ä¸º *red curve = constant + blue 'Z' curve*

<img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240923165416649.png" alt="image-20240923165416649" style="zoom: 33%;" />



:arrow_down:how to represent blue 'Z' curve?

**answer**: <u>we can use sigmoid function.</u>

<img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240923171230582.png" alt="image-20240923171230582" style="zoom: 33%;" />

ä¸ºäº†æ‹Ÿåˆçº¢è‰²æ›²çº¿ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç»„åˆå¤šä¸ªä¸åŒçš„ sigmoid å‡½æ•°å®ç°ï¼Œå¾—åˆ°å¦‚ä¸‹å¼å­ï¼š
$$
y=\sum_i{c_i\ \sigma(w_ix_1+b_i)} + b
$$
ä½†æ˜¯ä¸Šè¿°å¼ä»…è€ƒè™‘åˆ°ä¸€ä¸ªç‰¹å¾ï¼Œå½“æœ‰å¤šä¸ªç‰¹å¾æ—¶ï¼Œå…¶ model å¯ä»¥ä¿®æ”¹ä¸ºå¦‚ä¸‹å¼å­ï¼š
$$
y=\sum_i{c_i\ \sigma(\sum_{j}w_{ij}x_j+b_i)} + b
$$

> - $i$ denotes the no. of sigmoid functions
>
> - $j$ denotes the no. of features
> - $w_{ij}$ denotes the weight for $x_j$ for i-th sigmoid



:arrow_double_down:**è¿›ä¸€æ­¥è§£é‡Š**

å‡è®¾æˆ‘ä»¬å°†ç´¯åŠ çš„åŠ æƒè¡¨è¾¾å¼æ‹†å¼€ï¼Œæˆ‘å¯ä»¥å¯ä»¥å¾—åˆ° $r_1, r_2, r_3, \cdots$

![image-20240923175421725](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240923175421725.png)

æˆ‘ä»¬å°†ä¸Šå¼è½¬æ¢ä¸ºçŸ©é˜µè¡¨è¾¾å¼
$$
\begin{aligned}
& r_1=b_1+w_{11} x_1+w_{12} x_2+w_{13} x_3 \\
& r_2=b_2+w_{21} x_1+w_{22} x_2+w_{23} x_3 \\
& r_3=b_3+w_{31} x_1+w_{32} x_2+w_{33} x_3 \\
& \Downarrow
\\
& {\left[\begin{array}{l}
r_1 \\
r_2 \\
r_3
\end{array}\right]=\left[\begin{array}{l}
b_1 \\
b_2 \\
b_3
\end{array}\right]+\left[\begin{array}{lll}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23} \\
w_{31} & w_{32} & w_{33}
\end{array}\right]\left[\begin{array}{l}
x_1 \\
x_2 \\
x_3
\end{array}\right]} \\
& \Downarrow
\\
& \boldsymbol{r}=\boldsymbol{b}+W\boldsymbol{x}
\end{aligned}
$$
![image-20240923180712272](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240923180712272.png)

![image-20240923180752895](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240923180752895.png)

> ä¸Šè¿°æ¯ä¸€ä¸ª sigmoid ï¼ˆå¯¹åº”ä¸Šå›¾çš„è“è‰²å°çƒï¼‰æˆ‘ä»¬ç§°ä¹‹ä¸ºä¸€ä¸ªç¥ç»å…ƒï¼ˆneuronï¼‰ï¼Œæ•´ä¸ªæµç¨‹æˆ‘ä»¬ç§°ä¹‹ä¸ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå¾—åˆ°çš„ $a_i$ ä»ç„¶å¯ä»¥äº¤å‰ç›¸ä¹˜å¾—åˆ°åŠ æƒè¡¨è¾¾å¼ $r$ï¼Œç„¶åå†è¿›è¡Œä¸€æ¬¡ sigmoid æ“ä½œï¼Œå…¶ä¸­åŒä¸€å±‚æ¬¡çš„ç¥ç»å…ƒæ„æˆä¸€ä¸ªéšè—å±‚ï¼ˆhidden layerï¼‰

åœ¨è¡¨è¾¾å¼ $y=b+c^T \sigma(\boldsymbol{b}+W \boldsymbol{x})$ ä¸­ï¼Œ$b, \boldsymbol{c^T}, \boldsymbol{b}, \boldsymbol{W}$ éƒ½æ˜¯æˆ‘ä»¬è¦æ±‚è§£çš„è¶…å‚æ•°ï¼Œ æ‰€ä»¥å¯¹äºæŸå¤±å‡½æ•° $L(\theta)$ å³æ˜¯ç”±è¿™äº›å‚æ•°æ„æˆï¼Œæˆ‘ä»¬åˆ†åˆ«å¯¹å…¶è¿›è¡Œå¾®åˆ†ï¼Œåˆ©ç”¨æ¢¯åº¦ä¸‹é™ï¼ˆGBï¼‰è¿›è¡Œè°ƒå‚ä¼˜åŒ–å³å¯
$$
\underset{\text { gradient }}{\boldsymbol{g}}=\left[\begin{array}{c}
\left.\frac{\partial L}{\partial \theta_1}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^0} \\
\left.\frac{\partial L}{\partial \theta_2}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^0} \\
:
\end{array}\right]
$$
æˆ‘ä»¬å°†å…¶ç®€åŒ–ä¸º $\boldsymbol{g}=\nabla L\left(\boldsymbol{\theta}^0\right)$

é¢å¯¹å¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å¸¸è§çš„æ“ä½œä¸æ˜¯ä¸€æ¬¡æ€§å¤„ç†ï¼Œè€Œä¸æ˜¯å°†æ•´ä¸ªæ•°æ®é›† $N$ åˆ†å‰²ä¸ºè‹¥å¹²ä¸ª batchï¼Œç„¶åä¸æ–­ $\boldsymbol{\theta}$ å¾—åˆ°æ–°çš„ $\boldsymbol{g}$ï¼Œä¸€æ•´ä¸ªæµç¨‹ç»“æŸåï¼Œæˆ‘ä»¬ç§°ä¸ºä¸€ä¸ª epoch(çºªå…ƒ)ã€‚

> [!tip]
>
> 1 epoch = see all the batcher once.

![image-20240923183034250](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240923183034250.png)



å¯¹äºæ‹Ÿåˆ "Z" æ›²çº¿ï¼Œæˆ‘ä»¬å…¶å®é™¤äº† sigmoid å‡½æ•°ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘ä½¿ç”¨ Rectified Linear Unit(ReLU) ï¼Œå…¶è¡¨è¾¾å¼ä¸ºï¼š
$$
y=c\ max(0, b+wx)
$$
<img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240923203507182.png" alt="image-20240923203507182" style="zoom:50%;" />



**neural network åŸºæœ¬æ¶æ„**

![image-20241002160646481](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241002160646481.png)



# 4 colab

## 4.1 åŸºæœ¬ä»‹ç»

> Google Colaboratory, or Colab, is an as-a-service [version of Jupyter Notebook](https://www.techtarget.com/searchaws/video/Set-up-a-Jupyter-notebook-on-AWS-with-this-tutorial) that enables you to write and execute Python code through your browser.







## 4.2 ä½¿ç”¨æ–¹æ³•

ä½¿ç”¨æ–¹æ³•ï¼šæ‰“å¼€ Google Driveï¼Œç„¶ååˆ›å»º Colab æ–‡æ¡£åƒ Jupyter ä¸€æ ·æ­£å¸¸ä½¿ç”¨å³å¯ã€‚

1. å¦‚ä½•ä¿®æ”¹è¿è¡Œæ—¶ç¯å¢ƒï¼Ÿ

   [Runtime] â†’ [Change runtime type]

2. æœ‰ç”¨çš„ Linux å‘½ä»¤ï¼ˆåœ¨ Colab ä¸­ï¼‰

   - `ls`
   - `ls -l`
   - `pwd`
   - `mkdir <dirname>`
   - `cd <dirname>`
   - `gdown`: download files from google drive
   - `weget`: download files from the internet
   - `python <python_file>`: execute a python file



æŸ¥çœ‹è¢«åˆ†é…çš„ GPU å‹å·

```python
gpu_info = !nvidia-smi 
gpu_info = '\n'.join(gpu_info) 
if gpu_info.find('failed') >= 0: 
    print('Not connected to a GPU') 
else: 
    print(gpu_info)
```

```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |
| N/A   64C    P8              13W /  70W |      0MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

ä»ä¸Šé¢çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬è¢«åˆ†åˆ°çš„ GPU æ˜¯ T4

> [!note]
>
> å¯ç”¨ GPU çš„æ€§èƒ½æ’è¡Œä¸ºï¼šP100 > T4 > k80, åœ¨å¤§éƒ¨åˆ†æ—¶å€™ï¼Œæˆ‘ä»¬éƒ½æ˜¯è¢«åˆ†é…åˆ° k80



1. æŒ‚è½½è‡ªå·±çš„è°·æ­Œäº‘ç›˜

   ```python
   from google.colab import drive
   drive.mount('/content/drive')
   ```

2. è®­ç»ƒæ¨¡å‹

   ```python
   !python3 /content/drive/MyDrive/main.py \
   --data_root /content/drive/MyDrive/dataset/battery \
   --logdir /comtent/drive/MyDrive/ \ --
   resume \
   | tee /comtent/drive/MyDrive/result.txt -a
   ```

   - ç¬¬ä¸€è¡Œä¸»è¦æ˜¯è¿è¡Œæ¨¡å‹çš„ä¸»ç¨‹åº
   - `--data_root` æŒ‡å®šæ•°æ®é›†çš„æ ¹ç›®å½•
   - `--logdir` ç”¨äºæŒ‡å®šä¿æŒæ¨¡å‹çš„æ—¥å¿—ï¼ˆcheckpoint + tensorboardï¼‰
   - `--resume` è¡¨ç¤ºå¦‚æœæœ‰ checkpoint å°±åŠ è½½ checkpoint
   - `|` æ˜¯ç®¡é“ç¬¦ï¼Œç”¨äºæµå¼è¾“å…¥è¾“å‡º
   - `tee` å‘½ä»¤å°†è¾“å‡ºä¿å­˜åˆ°æ–‡ä»¶çš„åŒæ—¶è¾“å‡ºåˆ°å±å¹•ï¼Œ`-a` ä»£è¡¨è¿½åŠ æ¨¡å‹













# 5 PyTorch

## 5.1 åŸºæœ¬ä»‹ç»

### 5.1.1 ä»€ä¹ˆæ˜¯ PyTorch

- An open source **machine learning framework**
- A python package that provides two high-level features:
  - Tensor computation(like NumPy) with strong **GPU acceleration**
  - Deep neural networks built on a **tape-base automatic gradient** system 

**PyTorch :vs: TensorFlow**

|             | PyTorch     | TensorFlow             |
| ----------- | ----------- | ---------------------- |
| Developer   | Facebook AI | Google Brain           |
| Interface   | Python, C++ | Python, C++, JS, Swift |
| Debug       | easy        | difficult              |
| Application | Research    | Production             |

![image-20240930172307539](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240930172307539.png)



### 5.1.2 ä»€ä¹ˆæ˜¯ Tensor

Tensor(å¼ é‡)å³ä¸€ä¸ªé«˜ç»´çŸ©é˜µ

![image-20240930174609733](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240930174609733.png)

- 0-1 tensor: scalar
- 1-D tensor: vector
- 2-D tensor: matrix
- 3-D tensor: multi-matrix

Tensor å¯¹è±¡ä¸‰ä¸ªåŸºæœ¬å±æ€§ï¼š

1. `rank`: number of dimensions
2. `shape`: number of rows and columns
3. `type`: data type of tensor's elements

**Tensor Constructor**

1. from list/NumPy array

   ```python
   # construct a 2-D tensor
   x = torch.tensor([[1, 2], [3, 4]])
   
   # construct a 2-D tensor
   x = torch.tensor([[1, 2], [3, 4]])
   ```

2. Zero tensor

   ```python
   # construct a 2*2 matrix filled with zero
   x = torch.zeros([2, 2])
   ```

3. Unit tensor

   ```python
   # construct a 1*2*5 tensor
   x = torch.ones([1, 2, 5])
   ```



**Tensor operators**

1. `torch.squeeze`: Returns a tensor with all specified dimensions of `input` of size 1 removed. When `dim` is given, a squeeze operation is done only in the given dimension(s).(begin with 0)

   ```python
   torch.squeeze(input, dim=None) -> Tensor
   ```

   > [!note]
   >
   > `squeeze` å‡½æ•°ä¸»è¦ç”¨äºç»´åº¦å‹ç¼©ï¼Œé»˜è®¤æƒ…å†µä¸‹å°†å¤§å°ä¸º 1 çš„ç»´åº¦æŠ¹é™¤ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å‚æ•° dim æŒ‡å®šè¦åˆ é™¤çš„ç»´åº¦ 

   ```python
   >>> x = torch.tensor([[[1, 2, 3], [4, 5, 6]]])
   >>> x.size()
   torch.Size([1, 2, 3])
   
   >>> x = torch.squeeze(x)
   >>> x.size()
   torch.Size([2, 3])
   ```

2. `torch.unsqueeze`: Returns a new tensor with a dimension of size one inserted at the specified position. $dim \in [-input.dim()-1, input.dim()+1]$

   ![image-20240930210101666](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240930210101666.png)

   ```python
   torch.unsqueeze(input, dim) -> Tensor
   ```

   ç¤ºä¾‹ä»£ç 

   ```python
   >>> x = torch.tensor([1, 2, 3, 4])
   >>> y_1 = torch.unsqueeze(x, 0)
   >>> y_1, y_1.shape
   (tensor([[1, 2, 3, 4]]), torch.Size([1, 4]))
   
   >>> y_2 = torch.unsqueeze(x, 1)
   >>> y_2, y_2.shape
   (tensor([[1],
            [2],
            [3],
            [4]]),
    torch.Size([4, 1]))
   ```

   > [!note]
   >
   > è¯¥å‡½æ•°æ˜¯å‹ç¼©çš„åå‘æ“ä½œï¼Œç”¨äºå¯¹ tensor åœ¨æŒ‡å®šä½ç½®è¿›è¡Œæ‰©å±•

3. `tensor.transpose`: Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped.

   ```python
   >>> x = torch.randn(2, 3)
   >>> x
   tensor([[-0.3842,  1.4757, -1.3104],
           [-0.6988, -0.6478,  0.4628]])
   
   >>> torch.transpose(x, 0, 1)
   tensor([[-0.3842, -0.6988],
           [ 1.4757, -0.6478],
           [-1.3104,  0.4628]])
   ```

   >[!note]
   >
   >è¯¥å‡½æ•°ä¸»è¦ç”¨äºç»´åº¦è°ƒæ¢

4. `torch.cat`: concatenate multi tensors

   ```python
   >>> x = torch.zeros([2, 1, 3])
   >>> y = torch.zeros([2, 2, 3])
   >>> z = torch.zeros([2, 3, 3])
   >>> w = torch.cat([x, y, z], dim=1)
   >>> w.shape
   torch.Size([2, 6, 3])
   ```

5. basic operators

   - Addition: `z = x + y`
   - Subtraction: `z = x - y`
   - Power: `y = x.pow(2)`
   - Summation: `y = x.sum()`
   - Mean: `y = x.mean()`

6. get Attributes

   | PyTorch            | NumPy                |
   | ------------------ | -------------------- |
   | x.shape            | x.shape              |
   | x.dtype            | x.dtype              |
   | x.reshape / x.view | x.reshape            |
   | x.squeeze()        | x.squeeze()          |
   | x.unsqueeze(1)     | np.expand_dims(x, 1) |

   

**Tensor - device**

- default: tensors & modules will be computed with **CPU**

- CPU

  ```python
  x = x.to('cpu')
  ```

- GPU

  ```python
  x = x.to('cuda')
  
  # check if you are qualified to use GPU
  torch.cuda.is_available()
  ```



**calculate gradient**

1. $x=\left[\begin{array}{cc}
   1 & 0 \\
   -1 & 1
   \end{array}\right]$ , $z=\sum_i \sum_j x_{i, j}^2$
2. $\frac{\partial z}{\partial x_{i,j}}=2 x_{i, j}$
3. $\frac{\partial z}{\partial x}=\left[\begin{array}{cc}
   2 & 0 \\
   -2 & 2
   \end{array}\right]$

ä½¿ç”¨ PyTorch è¿›è¡Œè®¡ç®—

```python
x = torch.tensor([[1., 0.], [-1., 1.]], requires_grad=True)
z = x.pow(2).sum()
z.backward()
x.grad
```



### 5.1.3 ä½¿ç”¨æµç¨‹

![image-20241001174028890](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241001174028890.png)



`Dataset` æ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œé‡Œé¢æœ‰å¾ˆå¤šæ•°æ®é›†å¯ä»¥ç›´æ¥è°ƒç”¨ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰æ•°æ®é›†ï¼Œå¦‚æœæˆ‘ä»¬è¦è‡ªå®šä¹‰æ•°æ®é›†ï¼Œé‚£ä¹ˆéœ€è¦ç»§æ‰¿ Dataset, å¹¶ä¸”é‡å†™ `__getitem__()`

`DataLoader` ä¸»è¦ç”¨äºåŠ è½½æ•°æ®

```python
class MyDataset(Dataset):
    def __init__(self, data):
        self.data = data
    def __getitem__(self, idx):
        return self.data[idx]
    def __len__():
        return len(self.data)

my_dataset = MyDataset(file)
dataloader = DataLoader(my_dataset, batch_size, shuffle=True)  
```

![image-20241001174253049](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241001174253049.png)



### 5.1.4 Neural Network Layers

- Linear Layer(Fully-connected Layer)

  ```python
  nn.Linear(in_features, out_features)
  ```

  ![image-20241001175104819](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241001175104819.png)

  ![image-20241001175334181](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241001175334181.png)

  > $\boldsymbol{W}$ æ˜¯å…¨è¿æ¥å±‚çš„çŸ©é˜µ

  ```python
  # weight: the learnable weights of the module of shape
  layer = torch.nn.Linear(32, 64)
  # get shape of weight in layer
  layer.weight.shape
  # get shape of bias in layer
  layer.bias.shape
  ```

- Activation Functions

  - Sigmoid: `nn.sigmoid`
  - ReLu: `nn.ReLu`

- Loss Functions

  - Mean Squared Error(for linear regression): `nn.MSELoss()`
  - Cross Entropy(for classification): `nn.CrossEntropyLoss()`

- torch.optim

  - Stochastic Gradient Descent(SGD)

    ```python
    torch.optim.SGD(params, lr, momentum=0)
    ```



### 5.1.5 construct Neural Network

**build a neural network**

```python
import torch.nn as nn
class MyModel(nn.Module):
    # initialize your model & define layers
    def __init__(self):
        super(MyMoulde, self).__init__()
        self.net = nn.Sequential(
            # add a linear layer
            nn.Linear(10, 32),
            # deal with a sigmoid function
            nn.Sigmoid(),
            # add a linear layer again
            nn.Linear(32, 1)
        )
    # compute output of your NN
    def forward(self, x):
        return self.net(x)
```



**neural network training**

```python
dataset = MyDataset(file)                            # read data via Dataset
tr_set = DataLoader(dataset, 16, shuffle=True)       # put dataset into DataLoader
model = MyModel().to(device)                         # construct a model and move to device
criterion = nn.MSELoss()                             # set loss function
optimizer = torch.optim.SGD(model.parameters(), 0.1) # set optimizer
```



## 5.2 ç¯å¢ƒé…ç½®

æå‰å®‰è£…å¥½ Adaconda or Miniconda, é…ç½®å¥½ conda ç¯å¢ƒ

1. åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ python ç¯å¢ƒå¹¶è¿›å…¥

   ```shell
   conda create -n pytorch python
   
   conda activate pytorch
   ```

   > [!caution]
   >
   > ä¸è¦ä½¿ç”¨å›½å†…é•œåƒæºè¿›è¡Œåˆ›å»º pytorch ç¯å¢ƒ

2. è¿›å…¥ [PyTorch](https://pytorch.org/) å®˜ç½‘å®‰è£… PyTorchï¼Œæ ¹æ®æœ‰æ—  N å¡ã€å¼€å‘è¯­è¨€ã€å¼€å‘å¹³å°é€‰æ‹©ç›¸åº”çš„ command è¿›è¡Œå®‰è£…

3. éªŒè¯ torch æ˜¯å¦å®‰è£…æˆåŠŸï¼šè¿›å…¥ python å‘½ä»¤è¡Œï¼Œè¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼Œå¦‚æœæ²¡æœ‰æŠ¥é”™ä»£è¡¨å®‰è£…æˆåŠŸ

   ```python
   import torch
   # éªŒè¯ GPU æ˜¯å¦å¯ä»¥ä½¿ç”¨ï¼ˆä»…é™å¸¦æœ‰ N å¡å¹¶ä¸”å®‰è£…äº† cudaï¼‰
   torch.cuda.is_available()
   ```

4. åœ¨åˆ›å»ºçš„ PyTorch ç¯å¢ƒä¸­å®‰è£… Jupyter

   ```shell
   # è¿›å…¥ pytorch ç¯å¢ƒ
   conda activate pytorch
   # å®‰è£… conda ç¯å¢ƒä¸­ç”¨äºæ”¯æŒ jupyter çš„åŒ…
   conda install nb_conda
   ```

5. éªŒè¯ jupyter æ˜¯å¦å¯ä»¥ä½¿ç”¨

   ```shell
   jupyter notebook	
   ```



## 5.3 ä½¿ç”¨æ–¹æ³•

### 5.3.1 ä¸¤å¤§æ³•å®å‡½æ•°

1. `dir()`ï¼šèƒ½è®©æˆ‘ä»¬çŸ¥é“å·¥å…·åŒ…ä»¥åŠå·¥å…·åŒ…ä¸­çš„åˆ†éš”åŒºæœ‰ä»€ä¹ˆä¸œè¥¿

   ```shell
   import torch
   # æŸ¥çœ‹ cuda æ˜¯å¦å¯ç”¨
   torch.cuda.is_available()
   # dir æŸ¥çœ‹ torch å…·ä½“å·¥å…·åŒ…ä¸­æ‰€å«æœ‰çš„å†…å®¹
   dir(torch)
   # è¿›ä¸€æ­¥æŸ¥çœ‹ torch.cuda ä¸­æ‰€åŒ…å«çš„å†…å®¹
   dir(torch.cuda)
   ```

2. `help()`ï¼šèƒ½è®©æˆ‘ä»¬çŸ¥é“æ¯ä¸ªå·¥å…·æ˜¯å¦‚ä½•ä½¿ç”¨çš„ï¼Œå³å·¥å…·çš„ä½¿ç”¨æ–¹æ³•

   ```shell
   # æŸ¥çœ‹ torch.cuda ä¸­çš„ _device å‡½æ•°
   help(torch.cuda._device)
   ```



### 5.3.2 æ•°æ®é›†å¯¼å…¥

- `torch.utils.data.dataset`: An abstract class representing a Dataset. All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite `__getitem__()`
- `torch.utils.data.DataLoader`ï¼šData loader combines a dataset and a sampler, and provides an iterable over the given dataset.



Dataset çš„ä½¿ç”¨

1. å¼•å…¥ç›¸åº”çš„åº“

   ```python
   from torch.utils.data import Dataset
   from PIL import Image
   import os
   ```

2. æ„å»ºä¸€ä¸ªç»§æ‰¿ Dataset çš„ç±» MyData

   ```python
   class MyData(Dataset):
       def __init__(self, root_path, label_path):
           # root_path æ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹çš„çˆ¶çº§è·¯å¾„; label_path æ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹çš„è·¯å¾„
           self.root_path = root_path
           self.label_path = label_path
           # å›¾ç‰‡æ–‡ä»¶æ‰€åœ¨æ–‡ä»¶æ–‡ä»¶å¤¹çš„å®Œæ•´è·¯å¾„
           self.path = os.path.join(self.root_path, self.label_path)
           self.img_path = os.listdir(self.path)
   	
       # é‡å†™ __getitem__ æ–¹æ³•
       def __getitem__(self, idx):
           img_name = self.img_path[idx]
           img_item_path = os.path.join(self.path, img_name)
           img = Image.open(img_item_path)
           return img, self.label_path
   
       def __len__(self):
           return len(self.img_path)
   ```

3. å‡è®¾ç°åœ¨åœ¨ root è·¯å¾„ä¸‹æœ‰ä¸¤ä¸ªå­˜æ”¾å›¾ç‰‡é›†çš„æ–‡ä»¶å¤¹ï¼Œä¸€ä¸ªæ˜¯ antsï¼Œå¦ä¸€ä¸ªæ˜¯ beesï¼Œç„¶åæˆ‘ä»¬é€šè¿‡åˆ›å»ºçš„ MyData ç±»è¿›è¡Œè¯»å–æ•°æ®é›†ï¼Œå†è¿›è¡Œåˆå¹¶

   ```python
   if __name__ == '__main__':
       root_path = 'D:\\Download\\BaiduNetdisk\\dataset\\hymenoptera_data\\train'
       ants_label_path = 'ants'
       bees_label_path = 'bees'
       ants_dataset = MyData(root_path, ants_label_path)
       bees_dataset = MyData(root_path, ants_label_path)
       train_dataset = ants_dataset + bees_dataset
   ```

4. å¯¹åˆå¹¶çš„æ•°æ®è¿›è¡Œæ“ä½œ

   ```python
   img, label = train_dataset[100]
   len(train_dataset)
   ```



### 5.3.3 TensorBoard

TensorBoard æ˜¯ä¸€ç»„ç”¨äºæ•°æ®å¯è§†åŒ–çš„å·¥å…·ã€‚å®ƒåŒ…å«åœ¨æµè¡Œçš„å¼€æºæœºå™¨å­¦ä¹ åº“ Tensorflow ä¸­ã€‚TensorBoard çš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

- å¯è§†åŒ–æ¨¡å‹çš„ç½‘ç»œæ¶æ„
- è·Ÿè¸ªæ¨¡å‹æŒ‡æ ‡ï¼Œå¦‚æŸå¤±å’Œå‡†ç¡®æ€§ç­‰
- æ£€æŸ¥æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ä¸­æƒé‡ã€åå·®å’Œå…¶ä»–ç»„ä»¶çš„ç›´æ–¹å›¾
- æ˜¾ç¤ºéè¡¨æ ¼æ•°æ®ï¼ŒåŒ…æ‹¬å›¾åƒã€æ–‡æœ¬å’ŒéŸ³é¢‘
- å°†é«˜ç»´åµŒå…¥æŠ•å½±åˆ°ä½ç»´ç©ºé—´

:rocket: **å®‰è£… TensorBoard**

```shell
conda install -c conda-forge tensorboard
```

:pencil2:ä½¿ç”¨ tensorboradX æ•°æ®åº“æ¥è®­ç»ƒæ ‡é‡æ•°æ®

```python
from tensorboardX import SummaryWriter
# åˆ›å»º SummaryWriter å®ä¾‹ï¼Œå¹¶æŒ‡å®šæ—¥å¿—æ–‡ä»¶ä¿å­˜åœ¨ logs ç›®å½•ä¸‹
writer = SummaryWriter('logs')
# writer.add_image()
for i in range(100):
    # æ·»åŠ æ ‡é‡ï¼Œå…¶ä¸­ tag ä»£è¡¨æ ‡é¢˜ï¼Œç¬¬äºŒä¸ªå‚æ•° scalar_value ç›¸å½“äºåæ ‡è½´ä¸­çš„ yï¼Œç¬¬ä¸‰ä¸ªå‚æ•° global_step ç›¸å½“äºåæ ‡è½´ä¸­çš„ x
    writer.add_scalar('y=x', i, i)
writer.close()
```

å¯åŠ¨ tensorboard ï¼Œåœ¨æ§åˆ¶å°è¾“å…¥å¦‚ä¸‹å‘½ä»¤ï¼š

```shell
tensorboard --logdir=logs --port=6007
```

> [!tip]
>
> ä¿ç•™çš„æ—¥å¿—æ–‡ä»¶ä»ç„¶ä¼šæ˜¾ç¤ºåœ¨ç•™åœ¨ logs ä¸­ï¼Œå¦‚æœæƒ³è¦æ“¦é™¤ä¹‹å‰çš„æ‹Ÿåˆæ›²çº¿éœ€è¦å°†å¯¹åº”çš„æ—¥å¿—åˆ é™¤





:point_right: **ä½¿ç”¨ tensorboard å¤„ç†å›¾åƒâ€‹**

SummaryWriter å®ä¾‹çš„ `add_image()` çš„å‚æ•° img_tensor æœ‰ç‰¹å®šçš„é™åˆ¶ï¼Œå¦‚ä¸‹ï¼š

```python
img_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Image data
```

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ opencv è¿›è¡Œè¯»å–å›¾åƒï¼Œåœ¨è¿™ä¹‹å‰ï¼Œéœ€è¦ä¸‹è½½å®‰è£…å¥½ opencv åº“

```shell
pip install opencv-python
```

å®ç°ä»£ç å¦‚ä¸‹ï¼š

```python
from torch.utils.tensorboard import SummaryWriter
from PIL import Image
import numpy as np

writer = SummaryWriter('logs')
image_path = 'dataset/train/ants_image/0013035.jpg'
img_PIL = Image.open(image_path)
img_array = np.array(img_PIL)

writer.add_image("test", img_array, 1, dataformats='HWC')

writer.close()
```



### 5.3.4 transforms

transforms æ˜¯ä¸€ä¸ªå·¥å…·ç®±ï¼Œç”¨æ¥å°†æœªå¤„ç†çš„æ•°æ®å½¢å¼å¤„ç†æˆä¸ºæˆ‘ä»¬æœ€åæƒ³è¦çš„æ•°æ®å½¢å¼ã€‚

transforms ä½äºå·¥å…·åŒ… torchvision ä¸‹ï¼Œtorvision ä¸‹ä¸»è¦æœ‰ä¸‰ä¸ªæ¨¡å‹ï¼š

1. `torchvision.transforms`ï¼šæä¾›äº†å¸¸ç”¨çš„ä¸€ç³»åˆ—å›¾åƒé¢„å¤„ç†æ–¹æ³•ï¼Œä¾‹å¦‚æ•°æ®çš„æ ‡å‡†åŒ–ï¼Œä¸­å¿ƒåŒ–ï¼Œæ—‹è½¬ï¼Œç¿»è½¬ç­‰ã€‚
2. `torchvision.datasets`ï¼šå®šä¹‰äº†ä¸€ç³»åˆ—å¸¸ç”¨çš„å…¬å¼€æ•°æ®é›†çš„datasetsï¼Œæ¯”å¦‚MNISTï¼ŒCIFAR-10ï¼ŒImageNetç­‰ã€‚
3. `torchvision.model`ï¼šæä¾›äº†å¸¸ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚ AlexNetï¼ŒVGGï¼ŒResNetï¼ŒGoogLeNetç­‰ã€‚

:pencil2:**é€šè¿‡ transforms å·¥å…·ç®±å°†å›¾ç‰‡è½¬æ¢ä¸ºä¸€ä¸ª Tensor**

```python
from torchvision import transforms
from PIL import Image

img_path = 'D:\\Document\\PyProject\\pytorch\\dataset\\train\\ants_image\\0013035.jpg'
img = Image.open(img_path)

# ä½¿ç”¨ transforms åˆ›å»ºä¸€ä¸ª Tensor å¯¹è±¡
tensor_trans = transforms.ToTensor()
# è°ƒç”¨ Tensor å¯¹è±¡ï¼ˆå¯¹åº” __call__ å‡½æ•°ï¼‰ï¼Œä½¿å¾—å›¾ç‰‡è½¬æ¢ä¸º tensor
tensor_img = tensor_trans(img)
```



**:gift:é™„åŠ å‡½æ•°è§£æ**

1. `__call()__`ï¼šå±äºä¸€ç§ magic methodï¼Œä½œç”¨æ˜¯å°†ä¸€ä¸ªç±»çš„å®ä¾‹åŒ–å¯¹è±¡è½¬æ¢ä¸ºå¯è°ƒç”¨å¯¹è±¡

   ```python
   class Person:
       def __init__(self, name):
           self.name = name
   
       def __call__(self):
           print('Hello!', self.name)
   
   
   p = Person('john')
   # è°ƒç”¨æ–¹å¼ 1ï¼ˆrecommendï¼‰
   p()
   # è°ƒç”¨æ–¹å¼ 2
   p.__call__()
   ```

2. `transforms.ToTensor()`ï¼šå°† PIL å›¾åƒæˆ– ndarray è½¬æ¢ä¸ºä¸€ä¸ª tensor å¹¶ä¸”ç›¸åº”åœ°ç¼©æ”¾å®ƒçš„å€¼

   ```python
   import numpy as np
   from torchvision import transforms
   
   # ä½¿ç”¨ transforms.Compose å°†å¤šä¸ª transforms å‡½æ•°ç»„åˆä½¿ç”¨
   trans = transforms.Compose([transforms.ToTensor()])
   
   d1 = [1, 2, 3, 4, 5, 6]
   d2 = [4, 5, 6, 7, 8, 9]
   d3 = [7, 8, 9, 10, 11, 14]
   d4 = [11, 12, 13, 14, 15, 15]
   d5 = [d1, d2, d3, d4]
   d = np.array([d5, d5, d5], dtype=np.float32)
   
   d.shape # (3, 4, 6)
   b = trans(d)
   b.shape # torch.Size([6, 3, 4])
   ```

   > [!note]
   >
   > `transforms.ToTensor()` å°† PIL Image æˆ–è€… `numpy.ndarray (H x W x C)` è½¬æ¢ä¸ºä¸€ä¸ª `torch.FloatTensor(C x H x W)`ï¼Œå…¶ä¸­ C(channel) ï¼Œä»£è¡¨å›¾åƒçš„è‰²å½©é€šé“æ•°ç›®ï¼ŒH(Height)ï¼Œä»£è¡¨å›¾åƒçš„é«˜åº¦ï¼ŒW(Width)ï¼Œä»£è¡¨å›¾åƒçš„å®½åº¦

3. `transforms.resize()`ï¼š å°†è¾“å…¥çš„å›¾åƒè½¬æ¢ä¸ºæŒ‡å®šå°ºå¯¸å¤§å°

   ```python
   from torchvision import transforms
   from PIL import Image
   
   img = Image.open('D:\\Document\\PyProject\\pytorch\\dataset\\train\\ants_image\\0013035.jpg')
   
   print(img.size)  # (768, 512)
   
   trans_size = transforms.Resize((512, 512))
   img_resize = trans_size(img)
   print(img_resize.size)  # (512, 512)
   ```

4. `transforms.Normalize(mean, std)`ï¼šè¾“å…¥ `(C, H, W)` å½¢å¼çš„ tensorï¼Œå¯¹æ¯å±‚è¿›è¡Œæ ‡å‡†åŒ–åè¾“å‡ºï¼Œå³
   $$
   \text { output }[\text { channel }]=\frac{\text { input }[\text { channel }]-\text { mean }[\text { channel }]}{\operatorname{std}[\text { channel }]}
   $$
   



### 5.3.5 torvision ä¸­çš„æ•°æ®é›†

> [!tip]
>
> Reference: https://pytorch.org/vision/stable/datasets.html

**:rocket:ä½¿ç”¨ [CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10) æ•°æ®é›†è¿›è¡Œç»ƒæ‰‹**

```python
import torchvision

train_set = torchvision.datasets.CIFAR10(root='./datasets', train=True, download=True)
test_set = torchvision.datasets.CIFAR10(root='./datasets', train=False, download=True)

# æŸ¥çœ‹æµ‹è¯•é›†æ•°æ®
print(test_set[0])
# æŸ¥çœ‹è¯¥æ•°æ®é›†çš„ç±»åˆ«ï¼ˆå¯¹åº”çš„å°±æ˜¯æ ‡ç­¾æ ‡è®°ï¼‰
print(test_set.classes)
# å¯¹è®­ç»ƒé›†è¿›è¡Œè§£åŒ…
img, target = train_set[0]
# æ‰“å¼€å›¾ç‰‡
img.show()
```



### 5.3.6 DataLoader

> [!tip]
>
> Reference: https://pytorch.org/docs/stable/data.html

`Dataloader` ä¸»è¦ç”¨äºè£…è½½æ•°æ®ï¼Œå…¶å‡½æ•°ç­¾åå¦‚ä¸‹ï¼š

```python
DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None, *, prefetch_factor=2,
           persistent_workers=False)
```

:raising_hand_man:**ç¤ºä¾‹ä»£ç **

```python
import torchvision
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
# æ·»åŠ  transform å‚æ•°ç”¨äºå°† PIL æ ¼å¼çš„æ•°æ®è½¬æ¢ä¸º tensor
test_set = torchvision.datasets.CIFAR10(root='./datasets', train=False, download=True, transform=torchvision.transforms.ToTensor())

# åŠ è½½æ•°æ®ï¼Œbatch_size æ¯ä¸ª batch ä¸­åŠ è½½çš„æ•°æ®çš„ä¸ªæ•°, num_workers ä»£è¡¨ä½¿ç”¨çš„çº¿ç¨‹æ•°ï¼ˆå€¼ä¸º 0 ä»£è¡¨ä½¿ç”¨çš„æ˜¯ä¸»çº¿ç¨‹ï¼‰, drop_list æ˜¯å¦ä¿ç•™ç»è¿‡ batch å¤„ç†åå‰©ä½™çš„æ•°æ®
test_loader = DataLoader(dataset=test_set, batch_size=64, shuffle=True, num_workers=0, drop_last=True)

writer = SummaryWriter('logs')
step = 0
for data in test_loader:
    # è§£åŒ…è·å– data ä¸­çš„å€¼
    images, targets = data
    writer.add_images("test_data", images, step)
    step += 1

writer.close()
```



### 5.3.7 ç¥ç»ç½‘ç»œçš„åŸºæœ¬éª¨æ¶ - nn.Module çš„ä½¿ç”¨

> [!tip]
>
> Reference: https://pytorch.org/docs/stable/nn.html

:pencil2:**ç¤ºä¾‹ä»£ç **

```python
from torch import nn


# åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”¨äºå°†è¾“å…¥çš„å€¼åŠ ä¸€
class Model(nn.Module):
    # è°ƒç”¨çˆ¶ç±»çš„ init æ–¹æ³•è¿›è¡Œåˆå§‹åŒ–
    def __init__(self):
        super(Model, self).__init__()

    # @staticmethod
    def forward(self, in_put):
        out_put = in_put + 1
        return out_put


model = Model()
op = model.forward(1)
print(op)
```



### 5.3.8 convolution

> [!tip]
>
> Reference: https://pytorch.org/docs/stable/nn.html#convolution-layers

å¯¹ [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) çš„è§£æ

- Input: $(N, C_{in}, H_{in}, W_{in})$ or $(C_{in}, H_{in}, W_{in})$
- Output: $(N, C_{out}, H_{out}, W_{out})$ or $(C_{out}, H_{out}, W_{out})$


$$
\begin{aligned}
H_{\text{out}} &= \left\lfloor \frac{H_{\text{in}} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1 \right\rfloor \\
W_{\text{out}} &= \left\lfloor \frac{W_{\text{in}} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1 \right\rfloor
\end{aligned}
$$






:pencil2:**ç¤ºä¾‹ä»£ç **

```python
import torch
import torch.nn.functional as F

# è¾“å…¥å›¾åƒ
input_img = torch.tensor([
    [1, 2, 0, 3, 1],
    [0, 1, 2, 3, 1],
    [1, 2, 1, 0, 0],
    [5, 2, 3, 1, 1],
    [2, 1, 0, 1, 1]
])
# å·ç§¯æ ¸
kernel = torch.tensor([
    [1, 2, 1],
    [0, 1, 0],
    [2, 1, 0]
])

# é‡å¡‘è¾“å…¥å›¾åƒçš„ tensor, åœ¨å‚æ•° shape ä¸­ï¼Œå…ƒç»„çš„ç¬¬ä¸€ä¸ªå‚æ•°ä»£è¡¨ batch_size=1, ç¬¬äºŒä¸ªå‚æ•°ä»£è¡¨è‰²å½©é€šé“æ•°channel=1, ç¬¬ä¸‰ä¸ªå‚æ•°ä»£è¡¨å›¾ç‰‡é«˜åº¦Height=5, ç¬¬å››ä¸ªå‚æ•°ä»£è¡¨å›¾ç‰‡å®½åº¦Width=5
input_channel = torch.reshape(input_img, (1, 1, 5, 5))
kernel = torch.reshape(kernel, (1, 1, 3, 3))

# å¼€å§‹è¿›è¡Œå·ç§¯æ“ä½œ, stride æ§åˆ¶å·ç§¯æ ¸å·ç§¯çš„æ­¥é•¿
output = F.conv2d(input_channel, kernel, stride=1)
print(output)
```



:paperclip:**å·ç§¯è§£æ**

**Convolution animations**

> [!tip]
>
> *N.B.: Blue maps are inputs, and cyan maps are outputs.*

| No padding, no strides                                       | Arbitrary padding, no strides                                | Half padding, no strides                                     | Full padding, no strides                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![no_padding_no_strides](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/no_padding_no_strides.gif) | <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/arbitrary_padding_no_strides.gif" alt="arbitrary_padding_no_strides" style="zoom: 48%;" /> | <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/arbitrary_padding_no_strides_transposed.gif" alt="arbitrary_padding_no_strides_transposed" style="zoom: 54%;" /> | <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/full_padding_no_strides.gif" alt="full_padding_no_strides" style="zoom:45%;" /> |
| **No padding, strides=2**                                    | **Padding, strides=2**                                       | **Padding, strides (odd)**                                   |                                                              |
| ![no_padding_strides](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/no_padding_strides.gif) | <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/padding_strides.gif" alt="padding_strides" style="zoom: 67%;" /> | <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/padding_strides_odd.gif" alt="padding_strides_odd" style="zoom: 67%;" /> |                                                              |

> [!note]
>
> padding å‚æ•°ç”¨äºå¡«å……è¾“å…¥çš„å››è¾¹ï¼Œé»˜è®¤é›¶å¡«å……



**ç¤ºä¾‹ä»£ç ï¼šå°†å›¾ç‰‡è¿›è¡Œå·ç§¯æ“ä½œ**

```python
import torch
import torchvision as tv
from torch.utils.data import DataLoader
from torch import nn
from torch.utils.tensorboard import SummaryWriter


class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)

    def forward(self, x):
        x = self.conv(x)
        return x


def main():
    train_dataset = tv.datasets.CIFAR10('datasets', train=True, transform=tv.transforms.ToTensor(), download=True)
    dataloader = DataLoader(train_dataset, batch_size=64)
    writer = SummaryWriter('logs')
    m = Model()
    step = 0
    for data in dataloader:
        images, targets = data
        output = m(images)
        # shape ä¸­çš„ -1 ä»£è¡¨ batch_size è‡ªåŠ¨è°ƒæ•´
        output = torch.reshape(output, (-1, 3, 30, 30))
        writer.add_images('input', images, step)
        writer.add_images('output', output, step)
        step += 1
    writer.close()


if __name__ == '__main__':
    main()
```



### 5.3.9 pooling layers

> [!tip]
>
> Reference: https://pytorch.org/docs/stable/nn.html#pooling-layers

æ± åŒ–å±‚æœ‰å¾ˆå¤šå‡½æ•°å¯ä¾›æˆ‘ä»¬é€‰æ‹©ï¼Œæˆ‘ä»¬ä¸»è¦äº†è§£ [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d) è¯¥å¦‚ä½•ä½¿ç”¨



:gift:**ç©ºæ´å·ç§¯**â€‹

ç©ºæ´å·ç§¯ï¼ˆDilated Convolutionsï¼‰ä¹Ÿè¢«ç§°ä¸ºè†¨èƒ€å·ç§¯ï¼Œä¸»è¦æ˜¯ä¸ºäº†è§£å†³å›¾åƒåˆ†å‰²çš„é—®é¢˜ã€‚

ç©ºæ´å·ç§¯å¼•å…¥äº†ä¸€ä¸ª "æ‰©å¼ ç‡"(dilation rate)çš„è¶…å‚æ•°ï¼Œè¯¥å‚æ•°å®šä¹‰äº†å·ç§¯æ ¸å¤„ç†æ•°æ®æ—¶å„å€¼çš„é—´è·ã€‚

| <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/dilation.gif" alt="dilation" style="zoom:102%;" /> | <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240926205006924.png" alt="image-20240926205006924" style="zoom: 72%;" /> |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

---

å¯¹äº MaxPool2dï¼Œæˆ‘ä»¬ç”¨ä¸‹é¢çš„å›¾æ¥å±•ç¤ºå…¶è®¡ç®—è¿‡ç¨‹

![image-20240926214926669](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240926214926669.png)

**ç¤ºä¾‹ä»£ç **

```python
import torch
from torch.nn import MaxPool2d
from torch import nn


class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.MaxPool_1 = MaxPool2d(kernel_size=3, ceil_mode=True)

    def forward(self, x):
        return self.MaxPool_1(x)


def main():
    input_matrix = torch.tensor([
        [1, 2, 0, 3, 1],
        [0, 1, 2, 3, 1],
        [1, 2, 1, 0, 0],
        [5, 2, 3, 1, 1],
        [2, 1, 0, 1, 1]
    ], dtype=torch.float32)
    input_matrix = torch.reshape(input_matrix, (-1, 1, 5, 5))
    m = Model()
    output = m(input_matrix)
    print(output)


if __name__ == '__main__':
    main()
```



åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œç›¸é‚»çš„å·ç§¯å±‚ä¹‹é—´é€šå¸¸ä¼šæ·»åŠ ä¸€ä¸ªæ± åŒ–å±‚ï¼Œç”¨äºç¼©å°å‚æ•°çŸ©é˜µçš„å°ºå¯¸ï¼Œä»è€Œå‡å°‘æœ€åè¿æ¥å±‚ä¸­çš„å‚æ•°æ•°é‡ï¼Œæ‰€ä»¥åŠ å…¥æ± åŒ–å±‚å¯ä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦å’Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæœ€å…¸å‹çš„åº”ç”¨åœºæ™¯å°±æ˜¯å‹ç¼©å›¾ç‰‡ï¼ˆå°† 1024\*1024åƒç´ çš„å›¾ç‰‡å‹ç¼©åˆ°800\*800ï¼‰



### 5.3.10 éçº¿æ€§æ¿€æ´»

> [!tip]
>
> Reference:
>
> - ReLu: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU
> - Sigmoid: https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid

æˆ‘ä»¬å¸¸ä½¿ç”¨çš„ä¸¤ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°åˆ†åˆ«æ˜¯ï¼š

|                     `torch.nn.Sigmoid()`                     |                `torch.nn.ReLU(inplace=False)`                |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| $\operatorname{Sigmoid}(x)=\sigma(x)=\frac{1}{1+\exp (-x)}$  |         $\operatorname{ReLu}(x)=(x)^{+}=\max (0, x)$         |
| ![image-20240926233823171](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240926233823171.png) | <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240926233945856.png" alt="image-20240926233945856" style="zoom:105%;" /> |



### 5.3.11 Sequential

> [!tip]
>
> Reference: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#sequential

A sequential container. Modules will be added to it in the order they are passed in the constructor.





å°†ä¸‹å›¾çš„è¿‡ç¨‹ä½¿ç”¨ `Sequential` è¿›è¡Œè§£å†³

![image-20240927103408609](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20240927103408609.png)

**ä»£ç **

```python
import torch
from torch import nn
from torch.utils.tensorboard import SummaryWriter


class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.seq = nn.Sequential(
            nn.Conv2d(3, 20, 5, padding=0),
            nn.MaxPool2d(2),
            nn.Conv2d(20, 50, 5, padding=0),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(800, 500),
            nn.Linear(500, 10)
        )

    def forward(self, x):
        return self.seq(x)


def main():
    m = Model()
    input_data = torch.ones(64, 3, 28, 28)
    output_data = m(input_data)
    
    writer = SummaryWriter('logs')
    writer.add_graph(m, input_data)
    writer.close()
    

if __name__ == '__main__':
    main()
```

> [!tip]
>
> å¯¹äºäºŒç»´å·ç§¯è€Œè¨€ï¼Œç¡®å®š padding å€¼çš„å¤§å°å¯ç”¨ä¸Šè¿° $W_{out}$ æˆ– $H_{out}$ çš„å…¬å¼è¿›è¡Œåˆ¤æ–­



### 5.3.12 æŸå¤±å‡½æ•°ä¸åå‘ä¼ æ’­

> [!tip]
>
> Reference: https://pytorch.org/docs/stable/nn.html#loss-functions

- [`nn.L1Loss`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss)ï¼šä½¿ç”¨ L1 èŒƒå¼è®¡ç®—æŸå¤±
- [`nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)ï¼šä½¿ç”¨ L2 èŒƒå¼è®¡ç®—æŸå¤±

```python
import torch
from torch import nn

predict = torch.tensor([1, 2, 3], dtype=torch.float32)
ground_truth = torch.tensor([1, 2, 5], dtype=torch.float32)

loss_1 = nn.L1Loss()
loss_2 = nn.MSELoss()
result_1 = loss_1(predict, ground_truth)
result_2 = loss_2(predict, ground_truth)
```



# 6 Backpropagation

 GBï¼ˆGradient Descentï¼‰ çš„è¿è¡ŒåŸç†ï¼š

1. å‡å¦‚ Network parameters $\theta={w_1, w_2, \cdots, b_1, b_2, \cdots}$

2. ä¸æ–­è¿­ä»£å‚æ•°åˆ—è¡¨ï¼Œ$\theta^0 \longrightarrow \theta^1 \longrightarrow \theta^2 \longrightarrow \ldots \ldots$

   ![image-20241002191916349](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241002191916349.png)

   

å¯¹äº $\nabla \mathrm{L}(\theta)$ è€Œè¨€ï¼Œå®ƒé‡Œé¢çš„å‚æ•°å¯èƒ½æœ‰ä¸Šç™¾ä¸‡ä¸ªï¼Œæ‰€ä»¥è®¡ç®—å¼ç›¸å½“è€—è´¹æ—¶é—´çš„ï¼Œè€Œ backpropagation å°±æ˜¯ç”¨æ¥ä¼˜åŒ–è¿™ä¸€é—®é¢˜çš„

> [!tip]
>
> æ¢è¨€ä¹‹ï¼Œbackpropagation æœ¬è´¨å°±æ˜¯ä¼˜åŒ–è¿‡çš„æ¢¯åº¦ä¸‹é™ç®—æ³•



**:bulb:Chain Rule**

- Case 1: $y=g(x),\   z=h(y)$
  $$
  \frac{dz}{dx}=\frac{dz}{dy}\cdot \frac{dy}{dx}
  $$

- Case 2: $x=g(s), \ y=h(s), \ z=k(x, y)$
  $$
  \frac{d z}{d s}=\frac{\partial z}{\partial x} \frac{d x}{d s}+\frac{\partial z}{\partial y} \frac{d y}{d s}
  $$

---

![image-20241002203017870](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241002203017870.png)

å‡è®¾åœ¨ä¸€ä¸ª neural network ä¸­ï¼Œæƒé‡çŸ©é˜µä¸º $[w_1, w_2]$, åç½®é¡¹ä¸º $b$ï¼Œè¾“å‡ºå‡½æ•°ä¸º $z=x_1w_1+x_2w_2+b$ï¼Œä»£ä»·å‡½æ•° $C$

> [!note]
>
> cost function & loss function & objective function

$$
\frac{\partial{C}}{\partial{w}} = \frac{\partial{Z}}{\partial{w}} \cdot \frac{\partial{C}}{\partial{Z}}
$$

- Forward pass: compute $\frac{\partial{C}}{\partial{w}}$ for all parameters
- Backward pass: compute $\frac{\partial{C}}{\partial{Z}}$ for all activation function inputs z

![image-20241002211320057](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241002211320057.png)

å¯¹äº Forward pass è¿˜æ˜¯æ¯”è¾ƒå¥½ç®—çš„ï¼Œä¾‹å¦‚ $\frac{\partial{C}}{\partial{w_1}}=x_1$ï¼Œä½†æ˜¯å¯¹äº Backward pass å°±æ¯”è¾ƒéš¾è®¡ç®—ï¼Œæ­¤æ—¶æˆ‘ä»¬å°±éœ€è¦ä½¿ç”¨ä¸Šè¿°æ–¹å¼è¿›è¡Œè®¡ç®—

> [!caution]
>
> æ¨å¯¼å¾…ç»­...





# 7 logistic regression

![image-20241003113200929](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241003113200929.png)



# 8 ç±»ç¥ç»ç½‘ç»œè®­ç»ƒä¸èµ·æ¥å¦‚ä½•å¤„ç†

## 8.1 critical point

åœ¨ GB ä¸­ï¼Œæ­£æ˜¯ç”±äº Critical point çš„å­˜åœ¨ï¼Œä½¿å¾—æˆ‘ä»¬æ— æ³•å¾—åˆ°å‚æ•°çš„æœ€ä¼˜è§£

> [!tip]
>
> Critical point is also called **Stable point** or **Stationary point**

Critical point å¯ä»¥åˆ†ä¸ºéç‚¹ï¼ˆsaddle pointï¼‰å’Œæå€¼ç‚¹ï¼ˆextreme value pointï¼‰ï¼Œå…¶ä¸­æå€¼ç‚¹åˆæœ‰æå¤§å€¼ï¼ˆlocal maximumï¼‰å’Œæå°å€¼ï¼ˆlocal minimumï¼‰ï¼Œä»–ä»¬çš„å¤æ•°å½¢å¼ä¸º local maxima å’Œ local minima

|                         local minima                         |                         saddle point                         |                         local maxima                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004003530780.png" alt="image-20241004003530780" style="zoom:110%;" /> | <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004003725209.png" alt="image-20241004003725209" style="zoom:115%;" /> | <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004003738161.png" alt="image-20241004003738161" style="zoom: 120%;" /> |

åœ¨ GB ä¸­ï¼Œæˆ‘ä»¬çš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†ä½¿å¾— $loss \rightarrow 0$ï¼Œæ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬ä»…è€ƒè™‘ local minima å’Œ saddle point ä¸¤ç§æƒ…å†µï¼Œæ ¹æ®ä¸Šè¿°æƒ…å†µï¼Œå¦‚æœæˆ‘ä»¬é‡åˆ°çš„æ˜¯ local minimaï¼Œé‚£ä¹ˆå‘¨å›´å°±æ²¡æœ‰åŠæ³•å†ç»§ç»­ä¼˜åŒ–ï¼Œä½†å¦‚æœæˆ‘ä»¬é‡åˆ°çš„äº‹ saddle pointï¼Œé‚£ä¹ˆè™½ç„¶å‰åå·²ç»æ²¡æœ‰åŠæ³•ä¼˜åŒ–ï¼Œä½†æ˜¯å¯ä»¥åœ¨å·¦å³ä¸¤è¾¹è¿›è¡Œä¼˜åŒ–

---

:apple:**åˆ¤åˆ« saddle point å’Œ local minima**

å¯¹äºæŸä¸€ä¸ª modelï¼Œå®ƒçš„å‡½æ•°è¡¨è¾¾å¼ä¸º $L(\boldsymbol{\theta})$, å…¶åœ¨ $\boldsymbol{\theta}=\boldsymbol{\theta}^{\prime}$ å¤„çš„å€¼å¯ä»¥è¿‘ä¼¼ä»£è¡¨ä¸ºå¦‚ä¸‹å¼å­ï¼š
$$
L(\theta) \approx L(\textcolor{blue}{\theta'}) + \textcolor{green}{(\theta - \theta')^T \mathbf{g}} + \frac{1}{2} (\theta - \theta')^T \textcolor{red}{\mathbf{H}} (\theta - \theta')
$$

> [!tip]
>
> 1. $g$ ä»£è¡¨ $L(\boldsymbol{\theta}) åœ¨ $ $\boldsymbol{\theta}=\boldsymbol{\theta}^{\prime}$ å¤„çš„æ¢¯åº¦ï¼Œi.e., $\mathbf{g}=\nabla_\theta L\left(\theta^{\prime}\right)$
> 2. $H$ æ˜¯ $\boldsymbol{\theta}=\boldsymbol{\theta}^{\prime}$ å¤„çš„äºŒæ¬¡å¾®åˆ†æ—¶çš„ Hessian Matrix
> 3. ä¸Šè¿°å¼å­ä¸º Taylor å±•å¼€å¼çš„å‰ä¸‰é¡¹

å½“åœ¨ critical point å¤„æ—¶ï¼Œ$g=0$, è€Œç­‰å¼çš„ç¬¬ä¸€é¡¹ $L({\theta'})$ ä¸ºå¸¸æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦å¯¹ç¬¬ä¸‰é¡¹çš„å€¼è¿›è¡Œåˆ¤æ–­ï¼Œå‡è®¾ $v=\theta - \theta'$ï¼Œé‚£ä¹ˆ:

![image-20241004104544364](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004104544364.png)

å¦‚æœæ˜¯ saddle point, é‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥æ ¹æ® $H$ è¿›è¡Œè°ƒæ•´

å®é™…ä¸Šï¼Œæˆ‘ä»¬é‡åˆ° local minima çš„æƒ…å†µæ˜¯å°‘æ•°ï¼Œå¤§éƒ¨åˆ†éƒ½æ˜¯ saddle pointï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ‰¾åˆ°æ›´å¤šçš„æ–¹å‘å»æ›´æ–°ä¼˜åŒ–æˆ‘ä»¬çš„ loss

![image-20241004111906809](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004111906809.png)



## 8.2 Batch

åœ¨ optimization çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨ Batchï¼Œå³åœ¨ä¼˜åŒ–å‚æ•° $\theta$ çš„æ—¶å€™ï¼Œä¸ä¼šç›´æ¥æ‹¿å‡ºæ‰€æœ‰çš„è®­ç»ƒæ•°æ®ï¼Œç„¶åä¸€èµ·ä½¿ç”¨å»ä¼˜åŒ–æ¨¡å‹ï¼Œè€Œæ˜¯å°†è®­ç»ƒæ•°æ®åˆ‡å‰²ä¸ºè‹¥å¹²ä¸ª batchï¼Œç„¶åç”¨åä¸€ä¸ª batch å»ä¼˜åŒ–å‰ä¸€ä¸ª batch å¤„ç†è¿‡çš„ $\theta$ï¼Œä¸æ–­è¿™ä¹ˆè¿­ä»£ï¼Œè¿­ä»£ä¸€ä¸ªå‘¨æœŸæˆ‘ä»¬ç§°ä¹‹ä¸ºä¸€ä¸ª epochã€‚

![image-20241004113215473](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004113215473.png)

é‚£ä¹ˆï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆè¦ä½¿ç”¨ batchï¼Œä½¿ç”¨ batch æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ

å‡è®¾ example=20, æˆ‘ä»¬è®¾ç½® batch_size=20 å’Œ batch_size=1ï¼Œæƒ…å†µå¦‚ä¸‹

![image-20241004114800573](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004114800573.png)

å¯ä»¥çœ‹åˆ°

- å¯¹äº full batchï¼Œè®¡ç®—æ—¶é—´è¾ƒé•¿ä½†æ˜¯ "èµ°" åœ°æ›´åŠ å¹³ç¨³
- å¯¹äº batch_size=1ï¼Œèƒ½å¿«é€Ÿå¾—åˆ°æ¯ä¸€æ¬¡çš„å®éªŒç»“æœï¼Œä½†æ˜¯ "èµ°" åœ°æ›´åŠ æ›²æŠ˜



:bulb:æ¯”è¾ƒ batch è¾ƒå¤§å’Œè¾ƒå°çš„æƒ…å†µä¸‹æ‰€è€—è´¹æ—¶é—´çš„å®éªŒç»“æœ

- è¾ƒå¤§çš„ batch åœ¨ä¸€å®šèŒƒå›´å†…ï¼Œç”±äº GPU å…·æœ‰å¹¶è¡Œè¿ç®—çš„å¤„ç†èƒ½åŠ›ï¼Œæ‰€ä»¥æ¯æ¬¡æ›´æ–°æ‰€è€—è´¹çš„æ—¶é—´å¹¶ä¸ä¼šç‰¹åˆ«å¤§ï¼Œè¶…è¿‡å…¶å¹¶è¡Œè¿è¡Œå¤„ç†èƒ½åŠ›åï¼Œæ¯æ¬¡æ›´æ–°æ‰€è€—è´¹çš„æ—¶é—´ä¼šé™¡å‡

  ![image-20241004120532494](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004120532494.png)

- è¾ƒå°çš„ batch æ¯æ¬¡æ›´æ–°è€—è´¹çš„æ—¶é—´å°ï¼Œä½†æ˜¯å®Œæˆä¸€ä¸ª epoch æ‰€è€—è´¹çš„æ—¶é—´å¹¶ä¸å°ï¼Œåœ¨**æ§åˆ¶ sample çš„æ•°é‡ä¸å˜çš„æƒ…å†µ**ä¸‹ï¼Œ"Time for one update" - "batch size" å’Œ "Time for one epoch" - "batch size" çš„å˜åŒ–æ›²çº¿æ˜¯ç›¸åçš„

  ![image-20241004121017276](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004121017276.png)



:cocktail:ç»“è®ºï¼šè¾ƒå°çš„ batch å¹¶ä¸ä¼šæ¯”è¾ƒå¤§çš„ batch çš„æ—¶é—´å¼€é”€æ›´å°ï¼Œåè€Œæœ‰å¯èƒ½ä¼šæ›´å¤§

æ¯”è¾ƒ accuracy - batch_size çš„å˜åŒ–æ›²çº¿

![image-20241004150330825](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004150330825.png)

:thought_balloon:æ€è€ƒï¼šä¸ºä»€ä¹ˆ batch_size è¶Šå°ï¼Œæ¨¡å‹è¶Šç²¾ç¡®ï¼Œå¾€å¾€ä¼šæœ‰æ›´å¥½çš„æ€§èƒ½ï¼Ÿ

![image-20241004153430204](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004153430204.png)

ä¸Šé¢ç»™å‡ºä¸€å¼ å›¾æä¾›ä¸€ç§è§£é‡Šï¼Œbatch_size è¶Šå°ï¼Œåœ¨æ¯ä¸ª batch ä¸­ä¼šå½¢æˆè‡ªå·±çš„å‡½æ•°æ›²çº¿ï¼Œä½¿å¾—æœ€ç»ˆçš„ model æ›´åŠ å¹³æ»‘ï¼Œä¹Ÿå°±æ˜¯è¯´

<b style="color: orange">"Noisy" update is better for training.</b>

é‚£ä¹ˆï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ€è€ƒï¼Œsmall batch æ˜¯å¦åœ¨ testing dataset ä¸Šä¹Ÿæœ‰å¥½çš„ç»“æœå‘¢ï¼Ÿ

![image-20241004155555464](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004155555464.png)

å®éªŒè¯å® small batch åœ¨ testing dataset ä¸Šä¹Ÿæœ‰å¥½çš„ç»“æœï¼Œä¸Šè¿°æä¾›äº†ä¸€ç§è§£é‡Šï¼š

1. Training dataset å’Œ Testing dataset æ‹¥æœ‰ä¸åŒçš„æ•°æ®åˆ†å¸ƒ
2. å½“æˆ‘ä»¬é‡åˆ° Flat Minima æ—¶ï¼Œåœ¨ä¸¤ç±»æ•°æ®é›†ä¸Šçš„ loss ä¹‹å·®ä¸ä¼šå¤ªå¤§ï¼›ä½†æ˜¯å½“æˆ‘ä»¬é‡åˆ° Sharp Minima æ—¶ï¼Œä»å›¾ç¤ºä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ° loss ä¹‹å·®å°±éå¸¸å¤§
3. æˆ‘ä»¬ä½¿ç”¨ small batch æ›´å®¹æ˜“é‡åˆ° Flat Minima ï¼Œè€Œ large batch æ›´å®¹æ˜“é‡åˆ° Sharp Minima



## 8.3 Momentum

momentum å³ "åŠ¨é‡"ï¼Œåœ¨ (vanilla) Gradient Descent ç®—æ³•ä¸­ï¼Œ$\boldsymbol{\theta}$ ç§»åŠ¨çš„æ–¹å‘ç»å¸¸æ˜¯ $\nabla L(\theta)$ çš„åæ–¹å‘ï¼Œè€ŒåŠ ä¸Š momentum ä¹‹åï¼Œä½¿å¾— $\theta$ ç§»åŠ¨çš„æ–¹å‘æ˜¯ç»“åˆä¸Šä¸€æ­¥ç§»åŠ¨çš„åŠ¨é‡å’Œå½“å‰çš„ gradient

|                              GB                              |                        GB + Momentum                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004163853489.png" alt="image-20241004163853489" style="zoom:152%;" /> | ![image-20241004164001432](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004164001432.png) |

 

## 8.4 Adaptive Learning Rate

### 8.4.1 Adagrad

> [!warning]
>
> Training stuck â‰  Small Gradientï¼ˆè®­ç»ƒå¡ä½ä¸ä»£è¡¨æ¢¯åº¦å¾ˆå°ï¼‰

å½“æˆ‘ä»¬ä½¿ç”¨ GB ç®—æ³•ä¸æ–­è¿›è¡Œè¿­ä»£æ—¶ï¼Œloss ä¼šè¶Šæ¥è¶Šå°ï¼Œæœ€åå¡åœ¨æŸä¸ªåŒºé—´æ¥å›éœ‡è¡ï¼Œè¿™ä¸ªæ—¶å€™äººä»¬ä¼šè®¤ä¸ºå·²ç»åˆ°äº† critical pointï¼Œå…¶å®ä¸ç„¶ï¼Œå®é™…ä¸Šï¼Œgradient åªèƒ½è¶‹è¿‘ 0ï¼Œå¦‚æœæƒ³è¦çœŸæ­£è¾¾åˆ° 0 ä¸€èˆ¬æ˜¯ä¸å¯èƒ½çš„ï¼Œè¿™ä¸æˆ‘ä»¬è®¾ç½®çš„ learning rate $\eta$ æœ‰å…³

![image-20241004191625170](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004191625170.png)

ä½†æ˜¯å¦‚æœæˆ‘ä»¬å°† $\eta$ è®¾ç½®çš„éå¸¸å°ï¼Œåœ¨è¾ƒä¸ºå¹³æ»‘çš„æ›²é¢ä¸Šåˆä¼šéš¾ä»¥å‰è¿›ï¼Œå› æ­¤å¯¹ $\eta$ çš„è®¾ç½®ä¸èƒ½ one-size-fits-allï¼ˆä¸€åˆ€åˆ‡ï¼‰

æ‰€ä»¥æˆ‘ä»¬å°±å¯ä»¥å¼•å…¥ä¸€ä¸ªæ€æƒ³ï¼š<b style="color: orange">Different parameters needs different learning rate</b>

åœ¨å¡åº¦å¤§çš„åœ°æ–¹ï¼Œ$\eta$ å°±å˜å¤§ï¼›åœ¨å¡åº¦å°çš„åœ°æ–¹ï¼Œ$\eta$ å°±å˜å°

åŸæ¥æˆ‘ä»¬çš„ $\theta_i$  update è¡¨è¾¾å¼å¦‚ä¸‹ï¼š
$$
\boldsymbol{\theta}_i^{\boldsymbol{t}+\mathbf{1}} \leftarrow \boldsymbol{\theta}_i^{\boldsymbol{t}}-\eta\boldsymbol{g}_i^{\boldsymbol{t}}
$$

> [!tip]
>
> - $i$ï¼šmodel ç¼–å·
> - $t$ï¼šè¿­ä»£æ¬¡æ•°

ç°åœ¨ç»“åˆ **Root Mean Square** æ”¹å˜ update è¡¨è¾¾å¼
$$
\begin{aligned}
&\begin{gathered}
\boldsymbol{\theta}_i^{\boldsymbol{t}+\mathbf{1}} \leftarrow \boldsymbol{\theta}_i^{\boldsymbol{t}}-\frac{\eta}{\sigma_i^t}\boldsymbol{g}_i^{\boldsymbol{t}} \\
\sigma_i^t=\sqrt{\frac{1}{t+1} \sum_{k=0}^t\left(\boldsymbol{g}_i^{\boldsymbol{k}}\right)^2}
\end{gathered}\\
&\text { Used in Adagrad }
\end{aligned}
$$
å¯¹äºä¸Šå¼ï¼Œ$learning\ rate(lr) = \frac{\eta}{\sigma_i^t}\boldsymbol{g}_i^{\boldsymbol{t}} $ , å…¶ä¸­ $\eta$ æ˜¯å›ºå®šçš„ï¼Œ$g \uparrow \ \rightarrow \sigma\uparrow\ \rightarrow \ lr\downarrow$



### 8.4.2 â€‹RMSProp

RMSProp ï¼ˆroot mean square prop, å‡æ–¹æ ¹ä¼ é€’ï¼‰ä¹Ÿæ˜¯ä¸€ä¸ªè‡ªé€‚åº”å­¦ä¹ ç‡çš„ä¼˜åŒ–ç®—æ³•ï¼Œå®ƒä¸»è¦è§†ä¸ºè§£å†³ Adagrad åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œç”±äºå¡åº¦é€å¹³ç¼“ï¼Œå­¦ä¹ ç‡ $lr$ è¶‹è¿‘äº 0ï¼Œæ— æ³•ç»§ç»­æ¨è¿›çš„æƒ…å†µå‘ç”Ÿï¼Œæ‰€ä»¥æˆ‘ä»¬å¸Œæœ›æå‡ºä¸€ç§æ–¹æ³•æ¥åœ¨è¿™æ—¶å€™æ‰‹åŠ¨è°ƒæ•´ï¼Œå°†åŸæ¥çš„ $\sigma_t$ æ›´æ”¹ä¸ºå¦‚ä¸‹å¼å­ï¼š
$$
\begin{aligned}
& \sigma_i^{t}=\sqrt{\alpha\left(\sigma_i^{t-1}\right)^2+(1-\alpha)\left(g_i^t\right)^2} \\
& s.t.\  \alpha \in (0, 1)
\end{aligned}
$$

> [!tip]
>
> è¿™ä¸ª $\alpha$ æ˜¯ hyper parameterï¼Œæ˜¯æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨è°ƒæ•´çš„



### 8.4.3 Learning Rate Scheduling

![image-20241004210456383](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004210456383.png)

 ä¸Šè¿°è½¨è¿¹æ˜¯æˆ‘ä»¬ä½¿ç”¨ Adagrad è¿›è¡Œä¸æ–­è°ƒæ•´çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨æŸå‡ ä¸ªä½ç½®å¤„ï¼Œç”±äº $g$ çš„å‡å°ï¼Œt çš„çº¿æ€§å¢é•¿å¼€å§‹è¶…è¿‡æ¢¯åº¦çš„ç´¯ç§¯ï¼Œæ­¤æ—¶ $lr$ ä¼šå¢å¤§ï¼Œå¯¼è‡´ä¸Šä¸‹éœ‡è¡çš„æƒ…å†µå‘ç”Ÿï¼Œä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº† Learning Rate Schedulingï¼ˆå­¦ä¹ ç‡è°ƒåº¦ï¼‰

1. Learning Rate Decay: ç”±äºè¶Šåˆ°åé¢æˆ‘ä»¬è·ç¦»ç›®æ ‡çš„ critical point è¶Šè¿‘ï¼Œæ‰€ä»¥æˆ‘ä»¬è®¾è®¡ä¸€ä¸ªé€’å‡çš„å‡½æ•°ç»™ $\eta$
2. warm up: ç”±äºå¼€å§‹ä¸ç¡®å®šæ–¹å‘ï¼Œæ‰€ä»¥ $\eta$ æ•…æ„è®¾ç½®çš„å°ä¸€ç‚¹ï¼Œä¹‹åå†é€æ¸é€’å¢ï¼Œè¾¾åˆ°ä¸€ä¸ªä¸´ç•Œç‚¹æˆ–ä¸€å®šæ—¶é—´æ—¶ï¼Œå†å¼€å§‹ç¼“å‡

|                     Learning Rate Decay                      |                           Warm up                            |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004212920492.png" alt="image-20241004212920492" style="zoom:150%;" /> | <img src="https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004212915048.png" alt="image-20241004212915048" style="zoom:149%;" /> |



### 8.4.4 Final GB

å†ç»è¿‡å¤šé‡æ”¹è¿›åï¼Œæˆ‘ä»¬å¯¹æ¢¯åº¦ä¸‹é™çš„ update è¿‡ç¨‹æè¿°å¦‚ä¸‹ï¼š
$$
\boldsymbol{\theta}_i^{\boldsymbol{t}+\boldsymbol{1}} \leftarrow \boldsymbol{\theta}_i^{\boldsymbol{t}}-\frac{\eta^t}{\sigma_i^t} \boldsymbol{m}_i^{\boldsymbol{t}}
$$

- $\boldsymbol{m}$: momentum, last momentum ç»“åˆ current gradient

  > [!warning]
  >
  > è¿™é‡Œ $m$ çš„æ¨è¿›æ˜¯è€ƒè™‘åˆ°äº†æ–¹å‘çš„

- $\boldsymbol{\sigma}$: root mean square of gradients

  > [!warning]
  >
  > è¿™é‡Œçš„ $\boldsymbol{\sigma}$ ä»…ä»…åªä¸æ¢¯åº¦çš„æ•°å€¼æœ‰å…³



## 8.5 how to do classification

å…¶å®æˆ‘ä»¬å¯ä»¥ç”¨ Regression æ¥åš Classificationï¼Œæœ€å¿«æƒ³åˆ°çš„ä¸€ç§åŠæ³•å°±æ˜¯ regression ä¸æ˜¯æœ€åä¼šè¾“å‡ºä¸€ä¸ª scalar å—ï¼Ÿæˆ‘ä»¬æ±‚å‡ºè¿™ä¸ª scalar ä¸ç›®æ ‡ä¹‹é—´çš„è·ç¦»ï¼Œç¦»çš„è¿‘å°±å½’ä¸ºç›¸åº”çš„ç±»åˆ«ã€‚

ä½†æ˜¯è¿™æ ·åšä¸ä¸€å®šå‡†ç¡®ï¼Œå°¤å…¶æ˜¯å½“ç±»ä¸ç±»ä¹‹é—´å®Œå…¨æ— å…³æ—¶æ›´æ˜¯å¦‚æ­¤ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ **One-hot vector**ï¼ˆç‹¬çƒ­ç¼–ç ï¼‰è¡¨ç¤º ï¼š
$$
\hat{y} = 
\overset{\text{Class 1}}{
\begin{bmatrix} 
1 \\ 0 \\ 0 
\end{bmatrix}} 
\quad \text{or} \quad 
\overset{\text{Class 2}}{
\begin{bmatrix} 
0 \\ 1 \\ 0 
\end{bmatrix}} 
\quad \text{or} \quad 
\overset{\text{Class 3}}{
\begin{bmatrix} 
0 \\ 0 \\ 1 
\end{bmatrix}}
$$
å°†ä¸Šè¿°çš„ class è½¬æ¢ä¸º One-hot vector ä¹‹åï¼Œä¸‰ä¸ªç±»ä¹‹é—´å°±æ²¡æœ‰å…³ç³»ï¼Œå¹¶ä¸”ä¸¤ä¸¤ä¹‹é—´è·ç¦»ç›¸ç­‰ã€‚

åœ¨ Regression ä¸­ï¼Œæˆ‘ä»¬è¾“å‡ºéƒ½æ˜¯ä¸€ä¸ª scalarï¼Œç°åœ¨æˆ‘ä»¬åšåˆ†ç±»ä»»åŠ¡ï¼Œéœ€è¦æœ‰å¤šä¸ªè¾“å‡ºï¼Œå¯ä»¥å°†*è¾“å‡ºçš„å€¼*ä¸ *æƒé‡çŸ©é˜µ* ç›¸ä¹˜

![image-20241004232452819](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004232452819.png)
$$
\begin{aligned}
& \boldsymbol{y}=\boldsymbol{b}^{\prime}+W^{\prime} \sigma(\boldsymbol{b}+W \boldsymbol{x}) \\
& \widehat{\boldsymbol{y}}\dashleftarrow  \dashrightarrow\boldsymbol{y}^{\prime}=\operatorname{softmax}(\boldsymbol{y})
\end{aligned}
$$

> [!warning]
>
> åœ¨è¿™é‡Œï¼Œ$\hat{y}$ ä»£è¡¨çœŸå€¼ï¼Œ$y$ ä»£è¡¨é¢„æµ‹å€¼ï¼Œ$y^{\prime}$ ä»£è¡¨å°†é¢„æµ‹å€¼å½’ä¸€åŒ–åçš„å¤„ç†ç»“æœ



![image-20241004233459741](https://thinkbook16-blog-img.oss-cn-zhangjiakou.aliyuncs.com/img_for_typora/image-20241004233459741.png)

åœ¨è¿™é‡Œï¼ŒSoftmax ä½œç”¨ï¼š

1. å½’ä¸€åŒ–
2. å·®å¼‚åŒ–ï¼šå¤§çš„æ›´å¤§ï¼Œå°çš„æ›´å°

è‡³äºæœ€åè®¡ç®— **Loss of Classification**ï¼Œæˆ‘ä»¬æœ‰å¦‚ä¸‹ä¸¤ç§è®¡ç®—æ–¹å¼ï¼š

1. Mean Square Error(MSE): $e=\sum_i\left(\widehat{\boldsymbol{y}}_i-\boldsymbol{y}_i^{\prime}\right)^2$
2. Cross-entropy: $e=-\sum_i \widehat{y}_i \ln y_i^{\prime}$

ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œ**æˆ‘ä»¬éƒ½é€‰ Cross-entropy ä½œä¸ºæˆ‘ä»¬è®¡ç®— loss çš„æ–¹å¼**

> [!tip]
>
> åœ¨ PyTorch ä¸­ï¼ŒSoftmax å’Œ Cross-entropy æ˜¯ç»‘åœ¨ä¸€èµ·çš„ 



:heart_eyes_cat:ç‰¹åˆ«æé†’ï¼š**Minimize Cross-entropy å…¶å®å°±æ˜¯ maximize likelihood**ï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰

