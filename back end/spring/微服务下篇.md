[toc]

# 1 Spring Cloud 微服务组件

## 1.1 Sleuth / ZipKin

### 1.1.1 概述

Sleuth / ZipKin 是什么？作用是？

1. 在微服务框架中，一个由客户端发起的请求在后端系统中会经过多个不同的服务节点调用，来协同产生最后的请求结果，每一个请求都会形成一条复杂的分布式服务调用链路

   ![image-20230828171608298](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230828171608298.png)

2. 链路中任何一环出现高延时或者错误都会引起整个请求最后的失败，因此需要**对整个服务的调用进行链路追踪和分析**就非常重要，所以便引入了 Sleuth / ZipKin

3. Sleuth 和 Zipkin 简单关系图

   ![image-20230828174408039](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230828174408039.png)

   > 每个微服务中都集成 Sleuth ，将追踪信息交给 Zipkin 服务（收集和处理数据），Zipkin 提供 Dashboard 显示可视数据 

:tada:总结

1. Sleuth 提供了一套完整的**服务跟踪**的解决方案，并兼容 Zipkin
2. Sleuth 做链路追踪，Zipkin 做数据搜集 / 存储 / 可视化



### 1.1.2 Sleuth + Zipkin 原理

01 span 和 trace 在一个系统中使用 Zipkin 的过程

![image-20230828185838777](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230828185838777.png)

- 一条链路通过 Trace Id 唯一标识

- 请求信息由 Span Id 唯一标识，各 Span 通过 parent id 关联起来

  ![image-20230828190049111](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230828190049111.png)

- Trace：类似于树结构的 Span 集合，表示一条调用链路，存在唯一标识

- Span：基本工作单元，表示调用链路来源



### 1.1.3 搭建 Sleuth + Zipkin 环境

**01 下载使用 zipkin**

> 下载地址：https://repo1.maven.org/maven2/io/zipkin/java/zipkin-server/2.12.9/

下载完成后，将其放在指定目录，并打开 cmd ，输入以下命令运行

```bash
java -jar zipkin-server-2.12.9-exec.jar
```

本地访问地址：http://localhost:9411/zipkin/



**02 微服务中集成 sleuth + zipkin**

引入相关依赖

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-zipkin</artifactId>
</dependency>
```

> 注意事项
>
> 1. 这里没有写版本号，是由于使用了父工程 maven 的版本仲裁
> 2. 由于 zipkin 场景启动器中引入了 sleuth 依赖，所以不需要我们重复引入



**03 对 application.yml 进行配置**

```yaml
spring:
  # 配置 zipkin 和 sleuth
  zipkin:
    base-url: http://localhost:9411
  sleuth:
    sampler:
      # 配置采样率（在 0-1，其中 1 表示全部采集）
      probability: 1
```



**04 测试分析**

在本地浏览器中进行查看：http://localhost:9411/zipkin/

![image-20230828214903614](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230828214903614.png)

在可视化界面，可以查看到一次调用链路的深度，以及该链路包含的请求，各个请求的耗时，找到请求瓶颈，为优化提供依据



## 1.2 Nacos

> 官方文档：https://github.com/alibaba/Nacos

### 1.2.1 基本介绍

Nacos 的全称为 "Dynamic Naming and Configuration Service"，Nacos 的功能强大，可以用来替代注册中心 —— Eureka 和配置中心 —— Config

> Nacos 的架构理论基础是 CAP 理论，支持 AP 和 CP，可以自由切换，像 Eureka 只只能支持 AP 理论



### 1.2.2 下载和运行

环境要求：JDK1.8 、Maven3.2+

下载地址：https://github.com/alibaba/nacos/releases/tag/1.2.1

下载完后解压，并直接打开 bin 文件夹里面的 startup.cmd（如果是在 Linux 环境，则使用bash start.sh）

可视化界面直接在浏览器中打开即可：http://localhost:8848/nacos

> 默认的用户名：`nacos`，密码：`nacos`



### 1.2.3 集成 Nacos 到服务消费方和服务提供方

![image-20230828225927070](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230828225927070.png)



**实现步骤**

01 创建一个新的模块 member-service-nacos-provider-10004

02 配置 pom.xml，引入相关依赖

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>com.xzh</groupId>
        <artifactId>e-commerce-center</artifactId>
        <version>1.0-SNAPSHOT</version>
    </parent>

    <artifactId>member-service-nacos-provider-10004</artifactId>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <!--引入 nacos starter-->
        <dependency>
            <groupId>com.alibaba.cloud</groupId>
            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-actuator</artifactId>
        </dependency>

        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
        </dependency>

        <dependency>
            <groupId>org.mybatis.spring.boot</groupId>
            <artifactId>mybatis-spring-boot-starter</artifactId>
        </dependency>

        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>druid-spring-boot-starter</artifactId>
            <version>1.1.17</version>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-jdbc</artifactId>
        </dependency>

        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <artifactId>junit-vintage-engine</artifactId>
                    <groupId>org.junit.vintage</groupId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>com.xzh</groupId>
            <artifactId>e_commerce_center-common-api</artifactId>
            <version>1.0-SNAPSHOT</version>
        </dependency>
    </dependencies>

</project>
```



03 创建 application.yml 文件

```yaml
server:
  port: 10004

spring:
  application:
    name: member-service-nacos-provider
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource # 配置项目使用的数据源
    url: jdbc:mysql://localhost:3306/spring_cloud?useUnicode=true&characterEncoding=UTF-8&useSSL=true&serverTimezone=Asia/Shanghai
    username: root
    password: 486101620
  # 配置 nacos
  cloud:
    nacos:
      discovery:
        server-addr: localhost:8848  # 配置 nacos server 的地址
mybatis:
  mapper-locations: classpath:mapper/*.xml # 指定 mapper.xml 文件的位置
  type-aliases-package: com.xzh.springCloud.entity # 指定类型别名
# 配置暴露所有的监控点
management:
  endpoints:
    jmx:
      exposure:
        include: '*'
```



04 创建启动类

```java
// 注解 @EnableDiscoveryClient 启用的是 Nacos 的服务发现
@SpringBootApplication
@EnableDiscoveryClient
public class MemberNacosProviderApplication10004 {
    public static void main(String[] args) {
        SpringApplication.run(MemberNacosProviderApplication10004.class, args);
    }
}
```



05 测试

我们首先到 Nacos 提供的客户端平台 locahost:8848 去查看配置的服务显示在服务列表中

![image-20230828233830425](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230828233830425.png)



05 如法炮制，再次创建一个模块 member-service-nacos-provider-10006

06 创建服务消费方模块 member-service-nacos-consumer-82

![image-20230829102602078](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829102602078.png)



07 配置相应的 pom.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>com.xzh</groupId>
        <artifactId>e-commerce-center</artifactId>
        <version>1.0-SNAPSHOT</version>
    </parent>

    <artifactId>member-service-nacos-consumer-82</artifactId>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>com.alibaba.cloud</groupId>
            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-actuator</artifactId>
        </dependency>

        <dependency>
            <groupId>com.xzh</groupId>
            <artifactId>e_commerce_center-common-api</artifactId>
            <version>1.0-SNAPSHOT</version>
        </dependency>

        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
    </dependencies>
</project>
```



08 配置 application.yml

```yaml
server:
  port: 82
spring:
  application:
    name: member-service-nacos-consumer-82
  cloud:
    nacos:
      discovery:
        server-addr: localhost:8848
```



09 添加主启动类

```java
@SpringBootApplication
@EnableDiscoveryClient
public class MemberNacosConsumer82Application {
    public static void main(String[] args) {
        SpringApplication.run(MemberNacosConsumer82Application.class, args
        );
    }
}
```



10 添加配置类，使其支持 Ribbon + RestTemplate(Nacos 本身就直接支持了 Ribbon + RestTemplate 调用)，示例参考代码如下：

```java
@RestController
@Slf4j
public class MemberNacosConsumerController {
    // 这里的 IP 地址对应的就是注册到 Nacos 的服务名
    public static final String MEMBER_SERVICE_PROVIDER_URL = "http://member-service-nacos-provider";

    @Resource
    private RestTemplate restTemplate;

    @PostMapping("/member/nacos/consumer/save")
    public Result saveMember(@RequestBody Member member) {
        log.info("info = {}", member.toString());
        return restTemplate.postForObject(MEMBER_SERVICE_PROVIDER_URL + "/member/save/", member, Result.class);
    }

    @GetMapping("/member/nacos/consumer/get/{id}")
    public Result getMemberById(@PathVariable("id") Long id) {
        return restTemplate.getForObject(MEMBER_SERVICE_PROVIDER_URL + "/member/get/" + id, Result.class);
    }
}
```



11 补充：将默认的轮询算法替换为随机算法

```java
@Configuration
public class RibbonRule {
    @Bean
    public IRule myRibbonRule() {
        return new RandomRule();
    }
}
```

> 和之前一样，添加相应的配置类即可



### 1.2.4 CP 和 AP 说明

![20200811140959175_看图王](C:\Users\Administrator\Desktop\my_note\20200811140959175_看图王.png)



选择 AP 还是 CP？

- CP：服务可以不能用，但是必须要保证数据的一致性
- AP：数据可以短暂不一致，但最终是需要一致的，无论如何都要保证服务的可用
- 取舍：只能在 CP 和 AP 选择一个平衡点，大多数都是选择 AP 模式



Nacos 中 CP 和 AP 的相互切换

- Nacos 集群默认采用的是 AP 理论

- CURL 切换命令

  ```bash
  curl -X PUT '$NACOS_SERVER:8848/nacos/v1/ns/operator/switches?entry=serverMode&value=CP'
  ```

- URL 指令

  ```bash
  $NACOS_SERVER:8848/nacos/v1/ns/operator/switches?entry=serverMode&value=CP
  ```



## 1.3 Nacos 配置中心

我们为什么需要使用 Nacos 配置中心？

在微服务中，各个配置独立存在，彼此不共通，使用配置中心能解决**配置共享和配置动态更新**，并且有利于我们进行**配置管理**



### 1.3.1 实战演示

01 在 Nacos 提供的客户端管理页面新建配置 e-commerce-nacos-config-client-dev.yaml

```yaml
# 测试配置内容
config:
    ip: 122.22.22.22
    name: gugu
```

![image-20230829154128900](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829154128900.png)

> 由于是 yaml 文件，所以对于字符串是否包裹着双引号不做要求



02 创建新的模块 e-commerce-nacos-config-client-5000



03 配置 pom.xml ，引入相关依赖

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>com.xzh</groupId>
        <artifactId>e-commerce-center</artifactId>
        <version>1.0-SNAPSHOT</version>
    </parent>

    <artifactId>e-commerce-nacos-config-client-5000</artifactId>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>com.alibaba.cloud</groupId>
            <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
        </dependency>

        <dependency>
            <groupId>com.alibaba.cloud</groupId>
            <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-actuator</artifactId>
        </dependency>

        <dependency>
            <groupId>com.xzh</groupId>
            <artifactId>e_commerce_center-common-api</artifactId>
            <version>1.0-SNAPSHOT</version>
        </dependency>

        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
    </dependencies>
</project>
```



04 配置 bootstrap.yml 和 application.yml 

> 解释：bootstrap.yml 和 application.yml 都是用于给 spring boot 应用程序做配置文件的，但是不同的是在启动时，会先读取 bootstrap.yml，之后再读取 application.yml

```yaml
# bootstrap.yml
server:
  port: 5000
spring:
  application:
    # 这里的 name 需要参考 nacos 配置中心的 Data Id
    name: e-commerce-nacos-config-client
  # 配置 nacos
  cloud:
    nacos:
      # 注意：下面的服务注册中心地址和服务配置中心以后可能是位于不同主机上
      discovery:
        server-addr: localhost:8848 # 配置服务注册中心地址
      config:
        server-addr: localhost:8848 # 配置服务配置中心地址
        file-extension: yaml # 配置配置文件后缀名
```

```yaml
# application.yaml
spring:
  profiles:
    active: dev # 指定环境，常见的环境有 dev / test / prod
```

> 具体的查找步骤：根据 spring.cloud.nacos.config.server-addr 找到配置中心的地址，然后进行配置文件的拼接，为 `${spring.application.name} + ${spring.profiles.active} + ${spring.cloud.config.file-extension}`



05 配置主启动类 NacosConfigApplication5000

```java
@SpringBootApplication
@EnableDiscoveryClient
public class NacosConfigClientApplication5000 {
    public static void main(String[] args) {
        SpringApplication.run(NacosConfigClientApplication5000.class, args);
    }
}
```



06 配置 controller 进行测试使用

```java
@RestController
@Slf4j
public class NacosConfigClientController {
    /**
     * 注意事项
     * 1. 确定引入的 Value 注解所在包为 org.springframework.beans.factory.annotation
     * 2. config.ip 对应我们在 nacos 配置文件中的值，config.name 也是同样的道理，他们给到相应的属性
     */
    @Value("${config.ip}")
    private String configIp;
    @Value("${config.name}")
    private String configName;

    @GetMapping("/nacos/config/ip")
    public String getConfigIp() {
        return configIp;
    }

    @GetMapping("/nacos/config/name")
    public String getConfigName() {
        return configName;
    }
}
```



**注意事项**

1. Nacos 配置中心的参考文档：https://nacos.io/zh-cn/docs/quick-start-spring-cloud.html

2. 配置文件 application.yml 和 bootstrap.yml 结合会得到配置文件地址

   > spring boot 中配置文件的加载也是有优先级的，其中 bootstrap.yml 优先于 application.yml

3. 注意在 Nacos 配置中心中配置文件的后缀是 `.yaml`，而不是 `.yml`

4. 在项目初始化时，要保证从配置中心进行配置拉取，拉取配置后，才能保证项目的正常启动，也就是说如果项目不能正确地获取到 Nacos 配置中心的配置数据，项目会启动失败

5. `@RefreshScope` 是 Spring Cloud 的原生注解，实现配置信息自动刷新

   ```java
   @RestController
   @RefreshScope
   public class NacosConfigClientController {
       @Value("${config.ip}")
       private String configIp;
       @Value("${config.name}")
       private String configName;
       
       // ……
   }
   ```



### 1.3.2 Nacos 分类配置（配置隔离）



#### 1.3.2.1 方案1：Data Id

应用场景：某个微服务项目作为【开发 / 生产 / 测试】环境配置文件的自由切换



示意图

![image-20230829183724627](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829183724627.png)



![image-20230829185238912](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829185238912.png)

之后再切换 application.yml 中的 spring.profiles.active 即可

```yaml
spring:
  profiles:
    active: test
```



#### 1.3.2.2 方案2：Gruop 方案

应用场景：在一个分布式项目中，有着不同的开发小组，需要 Nacos Server 获取对应的开发环境配置的数据

![image-20230829191337771](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829191337771.png)

修改 bootstrap.yml 相关配置即可

```yaml
spring:
  cloud:
    nacos:
      config:
        server-addr: localhost:8848 # 配置服务配置中心地址
        file-extension: yaml # 配置配置文件后缀名
        group: order # 配置组别
```



#### 1.3.2.3 方案3：namespace 

应用场景：在一个分布式项目中，有多个组织 / 公司开发组，需要到 Nacos Server 获取开发华景 / 其他环境对应的配置数据

![image-20230829193318373](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829193318373.png)



配置命名空间

![image-20230829205535608](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829205535608.png)

![image-20230829211123595](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829211123595.png)



在 bootstrap.yml 中做相应的配置

```yaml
spring:
  cloud:
    nacos:
      config:
        server-addr: localhost:8848 # 配置服务配置中心地址
        file-extension: yaml # 配置配置文件后缀名
        group: order # 配置组名
        namespace: 45a1d04c-8c5e-4d0f-b936-088e9e0f1147 # 指定对应的 namespace
```



#### 1.3.2.4 梳理总结

![image-20230829211949378](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829211949378.png)

- Nacos 默认的命名空间是 public，namespace 主要用来实现配置隔离，隔离范围大
- gruop 默认值是 DEFAULT GROUP，gruop 可以吧不同的微服务划分到同一个分组里
- service 就是微服务，相同的 service 可以是一个 cluster（簇 / 集群），其中 instance 就是微服务的实例



## 1.4 sentinel

相关文档

1. github 仓库：https://github.com/alibaba/Sentinel

2. 中文手册

   - 地址1：https://sentinelguard.io/zh-cn/docs/quick-start.html
   - 地址2：https://github.com/alibaba/Sentinel/wiki/%E4%BB%8B%E7%BB%8D

3. 使用手册：https://spring-cloud-alibaba-group.github.io/github-pages/greenwich/spring-cloud-alibaba.html#_spring_cloud_al%20ibaba_sentinel

   

### 1.4.1 基本介绍

- 概述：**Sentinel 主要是用于分布式系统的流量防卫兵，保护微服务**

- 基本介绍：随着微服务的流行，服务与服务之间的稳定性变得越来越重要。Sentinel 以流量为切入点，从<u>流量控制、熔断降级、系统负载保护</u>等多个维度保护服务的稳定性

  > 主要就是解决分布式系统的 "三高"：高并发、高可用、高性能

**主要特性**

![image-20230829214453851](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829214453851.png)



**开源生态**

![image-20230829214516757](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829214516757.png)



**Sentinel 的核心功能**

1. 流量控制：即限流，拿生活中的事举例就比如，旅游某景点时会限制
2. 熔断降级：这其实是两个概念，即服务熔断和服务降级
   - 服务熔断：当调用链路中的某个资源不稳定，为避免出现服务不可用或者响应超时的情况时导致整个服务雪崩，暂时停止对该服务的调用
   - 服务降级：降低部分非核心功能的使用，保证核心功能的正常运行，对于这些非核心业务，通常会准备一个 fallback 来处理错误信息

3. 系统负载保护：根据系统能够处理的请求和允许进来的请求做平衡，追求的目标是在系统不被拖垮的情况下，提供系统的吞吐率
4. 消息削峰填谷：某时段迸发大量请求，而此时如果全部处理，可能会导致系统负载过高，影响稳定性，但是其实可能后面几秒之内都没有消息投递，如果直接将前面所有的消息丢掉则没有充分利用到系统处理消息的能力。Sentinel 的 Rate Limiter 模式能在某一段时间间隔内以匀速的方式处理这样的请求，充分利用系统的处理能力，形象地说就像是 "削峰填谷"，保证资源的稳定性



**Sentinel 的组成部分**

Sentinel 主要是由核心库和控制台两部分组成的

- 核心库：一个 Java 客户端，不依赖任何框架 / 库，能够运行在所有的 Java 运行时环境，对 Spring Cloud 有较好的支持
- 控制台：Dashboard，基于 Spring Boot 开发，打包后可以直接运行，不需要额外的 Tomcat 等应用容器



### 1.4.2 下载使用

> 下载地址：https://github.com/alibaba/Sentinel/releases/tag/v1.8.0

运行命令

```bash
java -jar sentinel-dashboard-1.8.0.jar
```

Dashboard 本地访问：http://localhost:8080

> 默认用户名和密码都是：sentinel



:spiral_notepad:备注：由于 8080 端口经常会与前端项目的端口产生冲突，因此为了避免这种情况的发生，我们可以直接其服务的端口

```bash
java -jar sentinel-dashboard-1.8.0.jar --server.port=9999
```



### 1.4.3 监控微服务

示意图

![image-20230829235040845](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230829235040845.png)



01 在 pom.xml 中引入 sentinel 场景启动器

```xml
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
</dependency>
```



02 在 application.yml 中添加相应配置

```yaml
spring:
  cloud:
    sentinel:
      transport:
        dashboard: localhost:8080 # 指定 sentinel 控制台的地址
        # 1. transport.port 端口配置会在被监控的微服务对应主机上启动 Http Server
        # 2. 该 Http Server 会与 Sentinel 控制台进行交互
        # 3. 例如 sentinel 控制台添加了一个限流规则，会把规则数据 push 到 Http Server 上，Http Server 再将这个规则注册到 Sentinel
        # 4. 简单而言 transport.port 是指定被监控的微服务与 Sentinel 控制台交互的端口
        # 5. 对于 transport.port ，有一套默认的机制，就是当默认的 8719 端口号被占用，就是以1步增直到找到可用的端口
        port: 8719
```



**注意事项**

1. QPS：全称为 "Queries Per Second"，代表的是每秒查询数，是服务器每秒响应的查询次数
2. Sentinel 采用的实时加载，只有调用了某个服务，才能看到监控数据



### 1.4.4 流控

在 Dashboard 中可以新增流控规则，如下图：

![image-20230830104130196](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830104130196.png)

对上图的解读：

- 资源名：唯一名称，默认为请求路径

- 针对来源：Sentinel 可以针对调用者进行限流，填写为服务名，默认为 default(不区分来源)

- 阈值类型 / 单机阈值

  - QPS：当调用该 api 的 QPS 达到阈值的时候就进行限流
  - 线程数：当调用该 api 的线程数达到阈值时就会进行限流

- 流控模式

  - 直接：api 达到限流条件时，直接限流
  - 关联：当关联的资源达到阈值时，就限流自己
  - 链路：当从某个接口过来的资源达到限流条件时，开启限流

- 流控效果

  - 快速失败：直接失败

  - warm up：根据 codeFactor（冷加载因子，默认 3）的值，即请求 QPS 从 threshold / 3 开始，经过预热时长，才达到设置的 QPS 阈值

    > warm up 翻译成就是 "热身"，目的是让并发的请求有一个渐进的过程，例如在秒杀服务中，大流量很容易冲垮系统

  - 排队等待：匀速排队，让请求以匀速的速度通过，阈值类型必须设置为 QPS，否则无效

**注意事项**

01 在配置流量规则时，我们可能会需要考虑到通配符的问题，例如对于 `/member/get/${id}` 这种资源，显然不可能单独为 `/member/get/1`，`/member/get/2`、…… 这样的一个个单独配置，解决方案有如下两种：

方案1：将路径参数修改为请求参数，例如请求资源为 `/member/get?id=1`、`/member/get?id=2`

```java
/**
 * 根据 id 查询 member 相关信息
 */
@GetMapping(value = "/member/get/", params = "id")
public Result queryMemberById(Long id) {
	// ……
}
```

如此以来我们可以直接对 `/member/get/` 进行流控



方案2：URL 资源清洗

使用原有的路径参数保持不变，可以通过 `UrlCleaner` 接口来实现资源清洗，也就是对 `/member/get/{id}` 这个 URL ，我们可以统一归集到 `/member/get/*` 资源下

```java
@Component
public class CustomUrlCleaner implements UrlCleaner {

    @Override
    public String clean(String s) {
        /**
         * StringUtils 所在的包为：org.apache.commons.lang.StringUtils
         * StringUtils.isBlank 用来判断字符串 s 是否存在以下三种情况
         * 1. 为 null; 2. 为 ""; 3. 为空格组成的字符串
         * 如果上述三种情况存在一种则返回 true, 否则返回 false
         */
        if(StringUtils.isBlank(s)) {
            return s;
        }

        if(s.startsWith("/member/get")) {
            return "/member/get/*";
        }
        
        return s;
    }
}
```

URL 清洗之后，我们可以在簇点链路上看到收集到的资源名为 `/member/get/*`，这样，我们就可以直接在该资源下进行设置



**02 关联关系实操**

设置限流规则，当 /t2 资源访问阈值达到设定的 QPS 1 时，访问 /t1 资源就会被限流

 ![image-20230830150316014](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830150316014.png)



### 1.4.5 排队

排队：这种方式其实也是限流的一种，通过严格控制请求通过的间隔时间，也即让请求以均匀的速度通过，对应的是漏桶算法

![image-20230830154318338](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830154318338.png)

> 这种方式主要用于处理间隔性突发的流量，例如消息队列。例如在某一秒有大量的请求到来，而接下来的几秒则处于空闲状态，我们希望系统能够在接下来的空闲时间内逐渐处理这些请求，而不是在第一秒直接拒绝多余的请求

注意：<u>如果是匀速排队，阈值的设置必须使用 QPS</u>



### 1.4.6 熔断降级

> 参考文档： https://sentinelguard.io/zh-cn/docs/circuit-breaking.html

除了流量控制以外，对调用链路中不稳定的资源进行熔断降级也是保障高可用的重要措施之一。一个服务常常会调用别的模块，可能是另外的一个远程服务、数据库，或者第三方 API 等。

![image-20230830160739663](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830160739663.png)

当所依赖的服务不稳定，会导致自身服务发生线程堆积的情况，最终耗尽业务自身的线程池，服务本身也变得不可用。因而我们需要对不稳定的**弱依赖服务调用**进行熔断降级，暂时切断不稳定调用，避免局部不稳定因素导致整体的雪崩。

#### 1.4.6.1 熔断、降级、限流三者之间的关系

- 熔断：强调的是服务之间的调用能够实现自我恢复的状态
- 限流：从系统的流量入口考虑，从进入的流量上进行限制，达到保护系统的作用
- 降级：是从系统业务维度考虑，流量大了或者频繁异常，可以牺牲一些非核心业务，保护核心业务的正常使用

> 总结：熔断是降级的一种，降级又是限流的一种，三者都是为了通过一定的方式在流量过大或者出现异常时，保护系统的手段



#### 1.4.6.2 熔断策略

**01 慢调用比例（SLOW_REQUEST_RATIO）**

![image-20230830164920977](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830164920977.png)

解释

- 选择以慢调用比例作为阈值，需要设置允许的慢调用 RT（即最大相应时间），若请求的响应时间大于该值则统计为慢调用

- 当<u>单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目</u>，并且<u>慢调用的比例大于阈值</u>，则接下来的的熔断时长内请求会自动被熔断

  > 结合配置列表，其实就是底部右边两个条件满足时则触发熔断机制

- 熔断时长后，熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求响应时间小于设置的慢调用 RT 则结束熔断，若大于设置的慢调用 RT 则会再次被熔断



**02 异常比例**

![image-20230830171834035](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830171834035.png)

1. 异常比例（ERROR_RATIO）：当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且异常的比例大于阈值，则接下来的熔断时长内请求会自动被熔断
2. 经过熔断时长后熔断器会进入探测回复状态（HALF-OPEN 状态）
3. 若接下来的一个<u>请求成功完成</u>（没有错误）则结束熔断，否则会再次被熔断
4. 异常比率的阈值范围是 [0.0, 1.0]，代表 0% - 100%

![image-20230830171813939](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830171813939.png)



**03 异常数**

![image-20230830172208618](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830172208618.png)

1. 异常数（ERROR_COUNT）：当单位统计时长内的异常数目超过阈值之后就会自动进行熔断
2. 经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态）
3. 若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断



### 1.4.7 热点规则

热点：即经常访问的数据。很多时候我们希望统计热点数据中，对访问频次最高的 Top K 数据进行限流，以防止系统在某段时间内高频访问导致系统雪崩

> ==热点规则== 与前面的不同之处在于，前面是直接对某个服务（接口）的请求进行限制，而这里是对某个服务（接口）指定参数的请求进行限制

![sentinel-hot-param-overview-1](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/sentinel-hot-param-overview-1.png)

- 热点参数限流会根据传入参数中的<u>热点参数</u>，并且根据<u>配置的限流阈值与模式</u>，对包含热点参数的资源调用进行限流

- Sentinel 利用 LRU 策略统计最近最常访问的热点参数，结合<strong style="color:red">令牌桶算法</strong>来进行参数级别的流控

  ![image-20230830180702692](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830180702692.png)



**添加热点规则**

01 

02 操作

![image-20230830200011426](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830200011426.png)

添加针对非热点参数设置的规则

![image-20230830201356865](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830201356865.png)

针对热点参数设置的规则（再次编辑）

![image-20230830201650795](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830201650795.png)



```java
@Slf4j
@RestController
public class MemberController {
    /**
     * 1. @SentinelResource: 指定 sentinel 限流资源
     * 2. value = "news" 表示 sentinel 限流资源名称，由程序员指定
     * 3. blockHandler = "newsBlockHandler" 表示触发限流后交给谁进行处理
     */
    @GetMapping("/news")
    @SentinelResource(value = "news", blockHandler = "newsBlockHandler")
    public Result queryNews(@RequestParam(value = "id", required = false) String id,
                            @RequestParam(value = "type", required = false) String type) {
        // 在实际开发中，新闻应该到 DB 或者缓存中获取
        log.info("到 DB 查询新闻");
        return Result.success("返回 id = " + id + " 新闻 from DB");
    }


    // 热点 key 限制
    public Result newsBlockHandler(String id, String type, BlockException blockException) {
        return Result.success("查询 id = " + id, "的新闻触发 key 热点保护，sorry…");
    }
 }
```



### 1.4.7 系统规则

**引出问题**

假如我们系统最大性能能抗 100 QPS，如何分配 `/t1`、`/t2` 的 QPS？

- 方案1：`/t1` 分配 QPS = 50，`/t2` 分配 QPS = 50
- 方案2：`/t1` 分配 QPS = 100，`/t2` 分配 QPS = 100

分析：方案1 不能充分利用服务器性能、方案2 则没有流量保护，造成请求线程堆积，形成雪崩

这时候就需要引出 ==系统规则== 来对各个资源请求的 QPS 进行弹性设置，使得总数不会超过系统最大  QPS 的流量保护规则



#### 1.4.7.1 基本介绍

![image-20230830215111952](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230830215111952.png)

- 系统处理请求的过程想象为一个水管，到来的请求是往这个水管灌水，当系统处理顺畅的时候，请求不需要排队，直接从水管中穿过，这个请求的 RT 是最短的；

- 反之，当请求堆积的时候，那么处理请求的时间则会变为：排队时间    +    最短处理时间



**系统规则**

- Load 自适应（仅对 Linux/Unix-like 机器生效）：系统的 load1 作为启发指标，进行自适应系统保护。当系统 load1 超过设定的启发值，且系统当前的并发线程数超过估算 的系统容量时才会触发系统保护（BBR 阶段）。系统容量由系统的 maxQps * minRt 估算得出。设定参考值一般是 CPU cores * 2.5。
- CPU usage（1.5.0+ 版本）：当系统 CPU 使用率超过阈值即触发系统保护（取值范围 0.0-1.0），比较灵敏。
- 平均 RT：当单台机器上所有入口流量的平均 RT 达到阈值即触发系统保护，单位是毫秒。
- 并发线程数：当单台机器上所有入口流量的并发线程数达到阈值即触发系统保护。
- 入口 QPS：当单台机器上所有入口流量的 QPS 达到阈值即触发系统保护。



### 1.4.8 全局限流处理类

在前面我们用到了一个注解 `@SentinelResource`，该注解用于指定 sentinel 限流资源，其中 blockHandler 参数用于指定异常处理方法

```java
@GetMapping("/news")
@SentinelResource(value = "news", blockHandler = "newsBlockHandler")
public Result queryNews(@RequestParam(value = "id", required = false) String id,
                        @RequestParam(value = "type", required = false) String type) {
    return Result.success("返回 id = " + id + " 新闻 from DB");
}

public Result newsBlockHandler(String id, String type, BlockException blockException) {
    return Result.success("查询 id = " + id, "的新闻触发 key 热点保护，sorry…");
}
```

但是上面的处理方案存在一些问题：

1. 每一个 `@SentinelResource` 对应一个异常处理方法，会造成方法很多
2. 异常处理方法和资源请求方法放在一起，不利于业务逻辑的分离
3. 解决方案：自定义全局限流处理类

```java
public class CustomGlobalBlockHandler {
    public static Result handlerMethod1(BlockException blockException) {
        return Result.error("400", "客户自定义异常/限流处理方法handlerMethod1");
    }

    public static Result handlerMethod2(BlockException blockException) {
        return Result.error("401", "客户自定义异常/限流处理方法handlerMethod2");
    }
}
```

```java
@RestController
public class MemberController {
    /**
     * blockHandlerClass 指定异常处理类
     * blockHandler 指定异常处理方法
     */
    @GetMapping("/t1")
    @SentinelResource(value = "t1", blockHandlerClass = CustomGlobalBlockHandler.class, blockHandler = "handlerMethod1")
    public Result t1() {
        return null;
    }
}
```



### 1.4.9 fallback

前面的 **blockHanlder 只负责 sentinel 控制台配置违规，而 fallback 主要负责 java 异常 / 业务异常**

```java
/**
 * @Description: 自定义全局异常处理类
 */
public class CustomGlobalFallback {
    public static Result fallbackHandlerMethod01() {
        return Result.error("403", "自定义 fallback 异常/fallbackHandlerMethod01");
    }

    public static Result fallbackHandlerMethod02() {
        return Result.error("403", "自定义 fallback 异常/fallbackHandlerMethod02");
    }
}
```

```java
@RestController
public class MemberController {
    /**
     * fallbackClass 指定 fallback 异常处理类
     * fallback 指定 fallback 异常处理方法
     */
    @GetMapping("/t1")
    @SentinelResource(value = "t1", fallbackClass = CustomGlobalFallback.class, fallback = "fallbackHandlerMethod01")
    public Result t1() {
        return null;
    }
}
```



### 1.4.10 @SentinelResource 注解的补充

`@SentinelResource` 这个注解除了前面的 fallbackClass 、CustomGlobalFallback、blockHandler 等配置之外，还有如下配置

- exceptionsToIgnore：用于忽略某些异常

  ```java
  @SentinelResource(value = "t1", exceptionsToIgnore = {RuntimeException.class})
  public Result t1() {
      return null;
  }
  ```



### 1.4.11 接入 Sentinel

接入 Sentinel 主要有两种方式：

1. 硬编码方式：侵入性强，不推荐
2. 注解方式：低侵入性，推荐

参考文档：https://sentinelguard.io/zh-cn/docs/annotation-support.html



### 1.4.12 openfeign + sentinel 远程调用

前面我们都是在单个服务上进行测试，但是实际上 sentinel 一般用于下游对上游的设置，示意图如下：

![image-20230831112552940](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230831112552940.png)

我们现在就是想要让 consumer 消费服务对 provider 提供服务方设置 sentinel，步骤如下：

01 添加 sentinel 和 openfeign 的场景启动器

```xml
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
</dependency>

<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
```



02 为 service 类（调用服务的类）创建 fallback 类（也可以创建 blockHandler 处理类）

```java
@Component
public class MemberFallbackService implements MemberOpenFeignService{
    @Override
    public Result queryMemberById(Long id) {
        return Result.error("500", "被调用服务异常，熔断降级，快速返回结果，防止线程堆积");
    }
}
```

> 这里需要注意：一定添加 `@Component` 将 fallback 类添加到容器中



03 同时在 service 中指定 fallback

```java
@FeignClient(value = "member-service-nacos-provider", fallback = MemberFallbackService.class)
public interface MemberOpenFeignService {
    /**
     * 远程调用方式是 get 方法，远程调用的 url 是 http://member-service-nacos-provider/member/get/{id}
     * member-service-nacos-provider 是 nacos 注册中心服务名
     * openfeign 会根据负载均衡算法来决定调用的使 10004 还是 10006
     */
    @GetMapping(value = "/member/get/{id}")
    public Result queryMemberById(@PathVariable("id") Long id);
}
```



04 最后不要忘记：需要在 applciation.yml 配置文件开启 sentinel

```yaml
feign:
  sentinel:
    enabled: true
```



05 设置完成后启动所需服务，可以发现在 sentinel 控制台中我们可以调用的上层服务进行限流或者熔断降级等操作

![image-20230831113931968](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230831113931968.png)



# 2 规则持久化

在前面我们发现，sentinel 设置的流控规则是没有持久化，当重启调用 API 所在的微服务后，规则就会丢失，需要重新加入

解决方案：<u>通过 Nacos 进行持久化</u>

> **补充：规则持久化的其他方案**
>
> - 阿里云 Ahas（最方便，但是付费）
> - 在 Nacos Server 配置规则完成持久化（官方推荐）
> - datasource 支持 nacos、redis、apollo、zk、file



## 2.1 工作原理示意图

![image-20230831124858887](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230831124858887.png)



## 2.2 Nacos 持久化

需求：为 member-service-nacos-consumer-82 微服务的 /member/openfeign/consumer/get/1 这个接口添加流控规则 QPS = 1，要求将该流控规则加入到 nacos server 配置中心，实现持久化

**操作步骤**

01 在 Nacos Server 配置中心增加 Sentinel 客户端 / 微服务模块的流控规则

dataId 自定义即可，我们在这里设置为 member-service-nacos-consumer-82 ，配置文件如下：

```json
[
    {
        "resource": "/member/openfeign/consumer/get/1",
        "limitApp": "default",
        "grade": 1,
        "count": 1,
        "strategy": 0,
        "controlBehavior": 0,
        "clusterMode": false
    }
]
```

> 对照前面的表格
>
> - resource: 资源名称
> - limitApp：来源应用
> - grade：阈值类型，0-线程数，1-QPS
> - count：单机阈值
> - strategy：流控模式，0-直接，1-关联，2-链路
> - controlBehavior：流控效果，0-快速失败，1-warm up，2-排队等待
> - clusterMode：是否集群



02 引入 sentinel 和 nacos 持久化依赖

```xml
<dependency>
    <groupId>com.alibaba.csp</groupId>
    <artifactId>sentinel-datasource-nacos</artifactId>
</dependency>
```



03 在 application.yml 做相应的配置

```yaml
spring:
  cloud:
    sentinel:
      datasource:
        ds1: # 自定义数据源名
          # 流控规则配置使从 nacos server 配置中心获取
          nacos:
            server-addr: localhost:8848 # 指定 nacos server 配置中心地址
            dataId: member-service-nacos-consumer-82 # 这里 dataId 与 nacos 配置中心的 dataId 对应
            groupId: DEFAULT_GROUP
            dataType: json # 配置流控规则的类型
            ruleType: flow # 表示规则类型是流控规则
```

> yaml 采用的是松散绑定，对于 lastName 驼峰命名，可以换成如下几种写法：`last_name`、`last-name`、`LASTNAME`



04 测试阶段，开启 Nacos、Sentinel、启动 provider 提供服务和 consumer 消费服务，看流控规则是否生效



**注意事项**

1. 在 nacos server 配置 sentinel 流控规则 Data Id 可以自己指定，只要保证和 sentinel client 里的 appication.yml 的 datasource.xxx.nacos.dataId 的值保持一致即可

2. 前面我们介绍了 sentinel 可以配置<u>流控规则、降级规则、热点规则和系统规则</u>四种，如果我们想要为其他三种规则持久化，我们需要改变 ruleType 为 degrade、param-flow、system，其他更多配置还需要参考文档和源码 https://sentinelguard.io/zh-cn/docs/basic-api-resource-rule.html

   ```java
   public enum RuleType {
       FLOW("flow", FlowRule.class),
       DEGRADE("degrade", DegradeRule.class),
       PARAM_FLOW("param-flow", ParamFlowRule.class),
       SYSTEM("system", SystemRule.class),
       AUTHORITY("authority", AuthorityRule.class),
       GW_FLOW("gw-flow", "com.alibaba.csp.sentinel.adapter.gateway.common.rule.GatewayFlowRule"),
       GW_API_GROUP("gw-api-group", "com.alibaba.csp.sentinel.adapter.gateway.common.api.ApiDefinition");
       //……
   }
   ```



# 3 Seata



## 3.1 场景引入

在原来的单机项目中，对于单库处理事务，我们可以采取以下两种方式：

1. 针对简单事务：提交 + 回滚
2. web 开发：Filter + ThreadLocal 进行事务管理

![image-20230831170156879](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230831170156879.png)

但是对于微服务，业务就非常复杂，每一个主机有自己的库，如果想要保证在多个 RPC 中保证数据一致性和可靠性，我们就可以考虑使用 Seata 组件解决这一问题

![architecture](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/architecture.png)



我们对上图进行梳理，后续会使用 Seata 对图中业务做一个简单的 demo

上图是一个用户购买商品的业务逻辑，整个业务逻辑由 3 个微服务提供支持：

- 账户服务（Accout）：从用户账户中扣除余额
- 订单服务（Order）：根据采购需要创建订单
- 仓储服务：对给定的商品扣除仓库 / 商品数量



## 3.2 基本介绍

> 官网：http://seata.io/zh-cn/

> 使用手册：https://seata.io/zh-cn/docs/overview/what-is-seata.html

概述：Seata 是一款开源的**分布式事务解决方案**，致力于在微服务架构下提供高性能和简单易用的分布式事务服务



## 3.3 下载使用

> 下载地址：https://github.com/seata/seata/releases/tag/v0.9.0

01 下载完成后，将压缩包解压到指定目录，同时修改 conf\file.conf

```ini
service {
  # 修改事务名
  vgroup_mapping.my_test_tx_group = "order_tx_group" 
}

store {
  ## 修改存储模式
  mode = "file"
  
  db {
    datasource = "dbcp"
    ## 指定数据库类型
    db-type = "mysql"
    ## 修改数据库连接驱动配置
    driver-class-name = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://127.0.0.1:3306/seata"
    user = "root"
    password = "123456"
  }
}
```

> 注意：这里我们修改驱动类为 com.mysql.cj.jdbc.Driver，但是 seata 的 lib 文件下没有 mysql 8 的驱动，只有 mysql 5 的驱动，所以需要我们手动替换，或者我们改用 mysql 5

02 创建 seata 数据库（对应 conf 文件），执行 seata 给我们的 sql 脚本

```sql
DROP DATABASE IF EXISTS seata;
CREATE DATABASE seata;
USE seata;
```

之后到 conf 文件夹下找到 db_store.sql，执行里面的 sql 语句即可



03 修改 seata 的 \conf\registry.conf，配置注册中心 nacos server

```ini
registry {
  # 修改使用的注册服务为 nacos
  type = "nacos"

  nacos {
    # serverAddr = "localhost"
    # 指定对应的服务地址
    serverAddr = "localhost:8848"
    namespace = ""
    cluster = "default"
  }
}
```



04 启动 nacos 8848，然后再双击 \bin\seata-server.bat（windows 系统）

我们来到 Nacos Server 控制台，如果 Seata 启动成功，那么在服务列表中会出现 serverAddr 

![image-20230831183815191](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230831183815191.png)



## 3.4 搭建环境

示意图

![image-20230831192151660](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230831192151660.png)



01 创建对应的 sql 表

```sql
-- 创建订单微服务的数据库
CREATE DATABASE order_micro_service;
USE order_micro_service;
CREATE TABLE `order`
(
    id         BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,
    user_id    BIGINT DEFAULT NULL,
    product_id BIGINT DEFAULT NULL,
    nums       INT    DEFAULT NULL,
    money      INT    DEFAULT NULL,
    `status`   INT    DEFAULT NULL COMMENT '0：创建中; 1：已完结'
);

-- 库存微服务的数据库`storage``order`
CREATE DATABASE storage_micro_service;
USE storage_micro_service;
CREATE TABLE `storage`
(
    id         BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,
    product_id BIGINT DEFAULT NULL,
    amount     INT    DEFAULT NULL COMMENT '库存量'
);
--    初始化库存表
INSERT INTO `storage` VALUES(NULL, 1, 10);

--  创建账户微服务的数据库
CREATE DATABASE account_micro_service;
USE account_micro_service;
CREATE TABLE `account`
(
    id      BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,
    user_id BIGINT DEFAULT NULL,
    money   INT    DEFAULT NULL COMMENT '账户金额'
);
--    初始化账户表
INSERT INTO `account` VALUES(NULL, 666, 10000);
```



02 创建对应的回滚日志表，该表在 seata 的 \conf\db_undo_log.sql

```sql
-- 这里以 storage_micro_service 为例，其他两个库也是同样的道理
USE storage_micro_service;
DROP TABLE IF EXISTS `undo_log`;
CREATE TABLE `undo_log`
(
    `id`            BIGINT(20)   NOT NULL AUTO_INCREMENT,
    `branch_id`     BIGINT(20)   NOT NULL,
    `xid`           VARCHAR(100) NOT NULL,
    `context`       VARCHAR(128) NOT NULL,
    `rollback_info` LONGBLOB     NOT NULL,
    `log_status`    INT(11)      NOT NULL,
    `log_created`   DATETIME     NOT NULL,
    `log_modified`  DATETIME     NOT NULL,
    `ext`           VARCHAR(100) DEFAULT NULL,
    PRIMARY KEY (`id`),
    UNIQUE KEY `ux_undo_log` (`xid`, `branch_id`)
) ENGINE = InnoDB
  AUTO_INCREMENT = 1
  DEFAULT CHARSET = utf8;
```



---

接下来，我们需要创建对应的微服务模块

03 创建 seata_storage_micro_service-10010 模块

04 在 pom.xml 中引入需要的依赖

```xml
<dependencies>
    <dependency>
        <groupId>com.alibaba.cloud</groupId>
        <artifactId>spring-cloud-starter-alibaba-seata</artifactId>
        <!--排除自带的 seata-all，引入自己的版本，否则会出现冲突-->
        <exclusions>
            <exclusion>
                <artifactId>seata-all</artifactId>
                <groupId>io.seata</groupId>
            </exclusion>
        </exclusions>
    </dependency>

    <!--引入指定版本的 seata-all, 这里因为我们的 seata 版本是0.9.0, 所以需要引入该版本依赖-->
    <dependency>
        <groupId>io.seata</groupId>
        <artifactId>seata-all</artifactId>
        <version>0.9.0</version>
    </dependency>

    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-openfeign</artifactId>
    </dependency>

    <dependency>
        <groupId>com.alibaba.cloud</groupId>
        <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
    </dependency>

    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>

    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-actuator</artifactId>
    </dependency>

    <dependency>
        <groupId>mysql</groupId>
        <artifactId>mysql-connector-java</artifactId>
    </dependency>

    <dependency>
        <groupId>org.mybatis.spring.boot</groupId>
        <artifactId>mybatis-spring-boot-starter</artifactId>
    </dependency>

    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>druid-spring-boot-starter</artifactId>
        <version>1.1.17</version>
    </dependency>

    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-jdbc</artifactId>
    </dependency>

    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
    </dependency>

    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
        <scope>test</scope>
        <exclusions>
            <exclusion>
                <artifactId>junit-vintage-engine</artifactId>
                <groupId>org.junit.vintage</groupId>
            </exclusion>
        </exclusions>
    </dependency>
</dependencies>
```



05 配置 applciation.yml

```yaml
server:
  port: 10010
spring:
  application:
    name: seata_storage_micro_service-10010
  cloud:
    alibaba:
      seata:
        # 指定事务组名（注意：需要和 seata-server 中对应的 /conf/file.conf 配置对应）
        tx-service-group: order_tx_group
    nacos:
      discovery:
        server-addr: localhost:8848
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://127.0.0.1:3306/storage_micro_service
    username: root
    password: 486101620
logging:
  level:
    io:
      # 配置 seata 日志
      seata: info
mybatis:
  mapper-locations: classpath:mapper/*.xml
```



06 在 resources 创建 file.conf，进行相关配置

> 说明：该文件直接从 \conf\file.conf 中拷贝，进行少量修改即可

![image-20230831211433617](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230831211433617.png)

结合在 nacos sever 中配置数据，主要修改内容如下：

```ini
service {
  # 修改组别
  vgroup_mapping.order_tx_group = "default"
  # 修改远程地址（这里由于在本地，直接使用 127.0.0.1 即可）
  default.grouplist = "127.0.0.1:8091"
}
```

> 这里千万要小心：vgroup_mapping 后面的 `order_tx_group` 需要和前面在 Seata 环境中配置的 file.conf 中的事务名保持一致



07 在 resources 下创建 registry.conf ，进行相关配置

> 该配置文件也是同样的道理，直接从 \seata\conf 中拿即可，然后做少量修改

```ini
registry {
  type = "nacos"

  nacos {
    serverAddr = "localhost:8848"
    namespace = ""
    cluster = "default"
  }
}
```



08 创建对应的实体类

```java
@Data
@AllArgsConstructor
@NoArgsConstructor
public class Storage {
    private Long id;
    private Long productId;
    private Integer amount;
}
```



09 创建 dao-service-controller



10 创建配置类

```java
// seata 代理数据源
@Configuration
public class DataSourceProxyConfig {
    // 注意：这里的 mapper-locations 必须要和配置文件参数一致，不能驼峰和短横线命名随意切换
    @Value("${mybatis.mapper-locations}")
    private String mapperLocations;

    // 配置 druidDatasource
    @Bean
    @ConfigurationProperties(prefix = "spring.datasource")
    public DataSource druidDataSource() {
        return new DruidDataSource();
    };

    // 配置 dataSourceProxy(io.seata.rm.datasource)
    @Bean
    public DataSourceProxy dataSourceProxy(DataSource dataSource) {
        return new DataSourceProxy(dataSource);
    }

    // 配置 sqlSessionFactory
    @Bean
    public SqlSessionFactory sqlSessionFactoryBean(DataSourceProxy dataSourceProxy) throws Exception {
        SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();
        sqlSessionFactoryBean.setDataSource(dataSourceProxy);
        sqlSessionFactoryBean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(mapperLocations));
        sqlSessionFactoryBean.setTransactionFactory(new SpringManagedTransactionFactory());
        return sqlSessionFactoryBean.getObject();
    }
}
```

```java
// 配置 mybatis
@Configuration
@MapperScan({"com.xzh.springCloud.dao"})
public class MyBatisConfig {
}
```

 

11 再以同样的道理创建 seata_account_micro_service-10012 和 seata_order_micro_service-10014

其中 seata_order_micro_service-10014 需要通过 openfeign 进行远程服务调用，在该模块 主要完成的业务如下：

1. 调用本地方法生成订单 order
2. 远程调用 storage 微服务扣减库存
3. 远程调用 account 微服务扣减用户余额
4. 调用本地方法修改订单状态 0 -> 1



12 在 orderService 中添加 `@GlobalTransactional` 进行全局事务管理

```java
@Service
@Slf4j
public class OrderServiceImpl implements OrderService {
    @Resource
    private OrderDao orderDao;

    @Resource
    private StorageService storageService;

    @Resource
    private AccountService accountService;

    /**
     * 1. @GlobalTransactional：分布式全局事务控制注解
     * 2. 参数 name 由程序员自己指定，但是需要保证唯一性
     * 3. 参数 rollbackFor 指定发生什么异常就进行回滚
     */
    @GlobalTransactional(name = "my-save-order", rollbackFor = Exception.class)
    @Override
    public void save(Order order) {
        // 1. 调用本地方法生成订单 order
        orderDao.save(order);
        // 2. 远程调用 storage 微服务扣减库存
        storageService.reduce(order.getProductId(), order.getNums());
        // 3. 远程调用 account 微服务扣减用户余额
        accountService.reduce(order.getUserId(), order.getMoney() * order.getNums());
        // 4. 调用本地方法修改订单状态 0 -> 1
        orderDao.update(order.getUserId(), 0);
    }
}
```



:bug:bug 收集

1. 如果出现 `Service id not legal hostname` 错误提示信息

   > 原因：Feign 的服务名不能使用下划线，可以使用短横线替换

2. MySQL 中出现 too many connections(1040) 错误

   > 解决方案：在 my.ini (mysql 配置文件)设置 `max_connections=1000`

3. 数据库 / 表在用到关键字时，需要使用反引号 ``，否则会报错，例如 order 表

   ```sql
   insert into `order` values(……)
   ```



## 3.5 分布式事务过程分析(ID + 三组件模型)

![solution](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/solution.png)

术语

- TC (Transaction Coordinator) ：事务协调者。维护全局和分支事务的状态，驱动全局事务提交或回滚。

- TM (Transaction Manager) ：事务管理器。定义全局事务的范围，开始全局事务、提交或回滚全局事务。

- RM (Resource Manager) ：资源管理器。管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。

执行过程

1. TM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的 XID
2. XID 在微服务调用链路的上下文中传播
3. RM 向 TC 注册分支事务，将其纳入 XID 对应全局事务的管辖
4. TM 向 TC 发起针对 XID 的全局提交或回滚协议
5. TC 调用 XID 下管辖的全部分支事务完成提交或回滚



![image-20230901152650816](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230901152650816.png)



梳理总结

![image-20230901163057772](https://new-blog-image.oss-cn-hangzhou.aliyuncs.com/img/image-20230901163057772.png)



## 3.6 seata 的事务模式

seata 一共有 4 种模式，分别是：

- AT（默认）
- TCC
- Saga
- XA

> 参考文档：https://seata.io/zh-cn/docs/overview/what-is-seata.html

